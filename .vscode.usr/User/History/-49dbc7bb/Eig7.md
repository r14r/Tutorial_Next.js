st.title("Chat mit Ollama (lokales LLM)")
---
# ï¿½ Tag 1 â€” EinfÃ¼hrung in KI & Ollama Grundlagen

## Einheiten-Index

- [Einheit 1.1 â€” Was ist KÃ¼nstliche Intelligenz (KI)?](./units/einheit-1-1.md)
- [Einheit 1.2 â€” Einstieg in Ollama](./units/einheit-1-2.md)
- [Einheit 1.3 â€” Ollama in der Kommandozeile (CLI)](./units/einheit-1-3.md)
- [Einheit 1.4 â€” Ollama API mit der Kommandozeile (cURL)](./units/einheit-1-4.md)
- [Einheit 1.5 â€” Zugriff Ã¼ber die API mit Python (Requests)](./units/einheit-1-5.md)
- [Einheit 1.6 â€” Nutzung der Ollama SDKs](./units/einheit-1-6.md)
- [Einheit 1.7 â€” Grundlagen der Arbeit mit LLMs](./units/einheit-1-7.md)
- [Einheit 1.8 â€” Prompt Engineering Grundlagen](./units/einheit-1-8.md)

### ğŸ“– Hintergrund

Ollama bringt ein eigenes Kommandozeilen-Tool mit. Damit kÃ¶nnen Modelle heruntergeladen, gelistet und direkt ausgefÃ¼hrt werden â€“ ohne Python.

- `ollama pull <modell>` â†’ Modell herunterladen
- `ollama list` â†’ verfÃ¼gbare Modelle anzeigen
- `ollama run <modell>` â†’ Modell starten und mit Prompts interagieren

### ğŸ’» Beispiele

- Modell herunterladen
```sh
ollama pull llama2
```

# 2. Alle installierten Modelle anzeigen
```
ollama list
```

# 3. Direkt mit einem Modell chatten
```
ollama run llama2
```

#### Dialog in der CLI

```
>>> Was ist Python?
Python ist eine Programmiersprache, die fÃ¼r ihre Einfachheit und Vielseitigkeit bekannt ist.
```

#### Direkter Prompt von der Shell aus

```sh
echo "Schreibe ein Haiku Ã¼ber KI." | ollama run llama2
```

### ğŸ“ Ãœbungen

1. Lade das Modell mistral herunter.
2. Liste alle Modelle und Ã¼berprÃ¼fe, ob mistral installiert ist.
3. FÃ¼hre `ollama run mistral` aus und frage nach: â€Nenne mir drei Anwendungen von KI.â€œ

### âœ… LÃ¶sungen

```sh
ollama pull mistral
ollama list
ollama run mistral
```

Antwortbeispiel:

- Sprachassistenten
- Bildklassifikation
- Betrugserkennung

---

## Einheit 1.4 â€” Ollama API mit der Kommandozeile (cURL)

### ğŸ“– Hintergrund

Ollama lÃ¤uft standardmÃ¤ÃŸig auf Port 11434 und bietet eine REST-API. Mit `curl` lassen sich Requests direkt aus der Shell stellen â€“ ideal zum Testen.

### ğŸ’» Beispiele

```sh
# 1. Gesundheitscheck
curl http://localhost:11434

# 2. Einfacher Chat mit llama2
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Nenne drei Programmiersprachen"
}'
```

Antwort-Beispiel (JSON):

```json
{"response": "Python, Java, C++"}
```

#### Chat-API Beispiel

```sh
curl http://localhost:11434/api/chat -d '{
  "model": "llama2",
  "messages": [{"role": "user", "content": "ErklÃ¤re maschinelles Lernen in einfachen Worten."}]
}'
```

### ğŸ“ Ãœbungen

1. Nutze curl, um eine Liste aller Modelle von der API abzurufen (`/api/tags`).
2. Sende mit curl einen Prompt: â€Schreibe ein Gedicht Ã¼ber Programmierer.â€œ

### âœ… LÃ¶sungen

```sh
# 1. Modell-Liste
curl http://localhost:11434/api/tags

# 2. Gedicht
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Schreibe ein Gedicht Ã¼ber Programmierer."
}'
```

---

## Einheit 1.5 â€” Zugriff Ã¼ber die API mit Python (Requests)

### ğŸ“– Hintergrund

FÃ¼r KI-Apps nutzen wir meistens Python, um die API aufzurufen. Das geht mit der Bibliothek `requests`. Vorteil: JSON einlesen, Fehlerbehandlung mÃ¶glich, Integration in Streamlit.

### ğŸ’» Beispiele

```python
import requests

# Einfacher Prompt
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": "Was ist KI?"})
print(r.json()["response"])
```

#### Mehrere Nachrichten (Chat)

```python
messages = [
    {"role": "system", "content": "Du bist ein hilfreicher Assistent."},
    {"role": "user", "content": "ErklÃ¤re neuronale Netze kurz."}
]

r = requests.post("http://localhost:11434/api/chat",
                  json={"model": "llama2", "messages": messages})
print(r.json()["message"]["content"])
```

#### Streaming von Antworten

```python
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": "ZÃ¤hle von 1 bis 5"},
                  stream=True)

for line in r.iter_lines():
    if line:
        print(line.decode(), end="")
```

### ğŸ“ Ãœbungen

1. Schreibe ein Python-Skript, das eine Frage an Ollama stellt und die JSON-Antwort ausgibt.
2. Implementiere eine Funktion `chat(prompt)`, die eine Antwort vom Modell zurÃ¼ckgibt.

### âœ… LÃ¶sungen

```python
# 1. Einfaches Skript
import requests
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": "Nenne 5 LÃ¤nder in Europa"})
print(r.json())

# 2. Funktion
def chat(prompt):
    r = requests.post("http://localhost:11434/api/generate",
                      json={"model": "llama2", "prompt": prompt})
    return r.json()["response"]

print(chat("Was ist Python?"))
```

---

## Einheit 1.6 â€” Nutzung der Ollama SDKs

### ğŸ“– Hintergrund

Neben der API bietet Ollama SDKs (z. B. fÃ¼r Python oder Node.js). Diese vereinfachen die Kommunikation und bieten oft komfortablere Methoden als reine HTTP-Requests.

### ğŸ’» Beispiele

#### Python SDK

```python
import ollama

# Einfacher Chat
response = ollama.chat(
    model="llama2",
    messages=[{"role": "user", "content": "ErklÃ¤re KI in 2 SÃ¤tzen"}]
)
print(response["message"]["content"])
```

#### Mehrere Nachrichten

```python
history = [
    {"role": "system", "content": "Du bist ein Tutor."},
    {"role": "user", "content": "Was ist Maschinelles Lernen?"}
]

response = ollama.chat(model="llama2", messages=history)
print(response["message"]["content"])
```

#### Liste der Modelle

```python
response = ollama.list()
for m in response.models:
    print("Modell:", m["model"])
```

#### Streaming im SDK

```python
response = ollama.chat(
    model="llama2",
    messages=[{"role": "user", "content": "ZÃ¤hle von 1 bis 5"}],
    stream=True
)

for chunk in response:
    print(chunk["message"]["content"], end="", flush=True)
```

### ğŸ“ Ãœbungen

1. Nutze das SDK, um eine Liste aller installierten Modelle auszugeben.
2. Schreibe ein kleines Chat-Skript, das eine Unterhaltung in einer Schleife ermÃ¶glicht.

### âœ… LÃ¶sungen

```python
# 1. Liste der Modelle
import ollama
response = ollama.list()
print([m["model"] for m in response.models])

# 2. Einfacher CLI-Chat
while True:
    user = input("Du: ")
    if user.lower() == "quit":
        break
    response = ollama.chat(model="llama2",
                           messages=[{"role": "user", "content": user}])
    print("Ollama:", response["message"]["content"])
```

---

## Einheit 1.7 â€” Grundlagen der Arbeit mit LLMs

### ğŸ“– Hintergrund

Large Language Models (LLMs) sind KI-Modelle, die auf Milliarden von Texten trainiert wurden. Sie lernen, das nÃ¤chste Wort in einem Satz vorherzusagen. Dadurch kÃ¶nnen sie Texte generieren, Fragen beantworten, zusammenfassen. Typische Modelle: GPT-4, Llama-2, Mistral. Ollama ermÃ¶glicht, diese Modelle lokal zu nutzen. Einsatz in KI-Apps: Chatbots, Textzusammenfassungen, Code-Helfer.

### ğŸ’» Code-Beispiele

#### Text-Zusammenfasser mit Ollama

```python
import streamlit as st
import requests

st.title("Dokument-Zusammenfassung mit Ollama")
file = st.file_uploader("Textdatei hochladen", type=["txt"])

if file:
    text = file.read().decode()
    prompt = f"Fasse den folgenden Text in 5 SÃ¤tzen zusammen:\n\n{text}"
    response = requests.post("http://localhost:11434/api/generate",
                             json={"model": "llama2", "prompt": prompt})
    summary = response.json()["response"]
    st.subheader("Zusammenfassung")
    st.write(summary)
```

### ğŸ“ Ãœbungen

1. ErklÃ¤re, warum LLMs nicht immer richtige Antworten geben (Halluzinationen).
2. Schreibe ein Skript, das eine Frage an Ollama stellt und die Antwort ausgibt.

### âœ… LÃ¶sungen

- LLMs basieren auf Wahrscheinlichkeiten und erfinden manchmal plausible, aber falsche Inhalte â†’ â€Halluzinationenâ€œ.

```python
import requests
frage = "Was ist die Hauptstadt von Frankreich?"
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": frage})
print("Antwort:", r.json()["response"])
```

---

## Einheit 1.8 â€” Prompt Engineering Grundlagen

### ğŸ“– Hintergrund

Prompt Engineering = die Kunst, gute Eingaben fÃ¼r ein Sprachmodell zu formulieren. Unterschiedliche Formulierungen fÃ¼hren zu anderen Ergebnissen. Ein guter Prompt enthÃ¤lt:

- klare Anweisung
- Kontext
- gewÃ¼nschtes Format

Beispiel: â€Fasse den Text zusammenâ€œ vs. â€Fasse den Text in 3 Bullet Points zusammenâ€œ.

### ğŸ’» Code-Beispiele

#### Prompt Playground in Streamlit

```python
import streamlit as st
import requests

st.title("Prompt Playground")
prompt = st.text_area("Prompt eingeben", height=150)

if st.button("Abschicken") and prompt:
    response = requests.post("http://localhost:11434/api/generate",
                             json={"model": "llama2", "prompt": prompt})
    st.subheader("Antwort")
    st.write(response.json()["response"])
```

### ğŸ“ Ãœbungen

1. Erstelle drei verschiedene Prompts, die denselben Text zusammenfassen, aber mit unterschiedlichem Stil (kurz, ausfÃ¼hrlich, Bulletpoints).
2. Teste mit Ollama, wie sich die Ergebnisse unterscheiden.

### âœ… LÃ¶sungen

- Prompt 1: â€Fasse diesen Artikel in 1 Satz zusammen.â€œ
- Prompt 2: â€Schreibe eine detaillierte Zusammenfassung in 5 SÃ¤tzen.â€œ
- Prompt 3: â€Fasse den Artikel in Bulletpoints zusammen.â€œ

---

## ğŸ¯ Zusammenfassung Tag 1 (erweitert)

Nach diesem Tag kÃ¶nnen die Studierenden:

- erklÃ¤ren, was KI, ML und LLMs sind,
- Ollama lokal installieren und erste Modelle nutzen,
- Ollama Ã¼ber die CLI bedienen (`pull`, `list`, `run`),
- Ollama Ã¼ber die REST-API mit `curl` ansprechen,
- Ollama in Python-Apps mit `requests` einbinden,
- Ollama komfortabel Ã¼ber das SDK nutzen (inkl. Streaming),
- einfache KI-Apps mit Streamlit und Ollama bauen,
- Prompts gezielt formulieren, um bessere Ergebnisse zu erhalten.
