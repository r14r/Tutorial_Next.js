## Einheit 1.2 ‚Äî Einstieg in Ollama

### üìñ Hintergrund

Ollama ist eine Open-Source-Plattform, die es erm√∂glicht, gro√üe Sprachmodelle lokal auszuf√ºhren. Modelle k√∂nnen heruntergeladen werden (`ollama pull`). Man kann mit ihnen √ºber eine API oder die Python-Bibliothek interagieren. Vorteil: Datenschutz (alles l√§uft lokal), keine API-Kosten, offline nutzbar. Das macht Ollama ideal f√ºr schnelles Prototyping von KI-Apps.

### üíª Code-Beispiele

#### Streamlit-App: Chat mit Ollama

```python
import streamlit as st
import requests

st.title("Chat mit Ollama (lokales LLM)")

if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []

user_input = st.text_input("Du:", "")

if st.button("Senden") and user_input:
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "llama2", "prompt": user_input}
    )
    reply = response.json()["response"]
    st.session_state['chat_history'].append(("Du", user_input))
    st.session_state['chat_history'].append(("Ollama", reply))

for sender, msg in st.session_state['chat_history']:
    st.write(f"**{sender}:** {msg}")
```

#### API Health Check

```python
import requests

try:
    r = requests.get("http://localhost:11434")
    print("‚úÖ Ollama API l√§uft!")
except Exception as e:
    print("‚ùå Verbindung fehlgeschlagen:", e)
```

#### Installationshinweis

```sh
# macOS
brew install ollama

# Modelle laden
ollama pull llama2

# Server starten
ollama serve
```

### üìù √úbungen

1. Installiere Ollama lokal und lade das Modell llama2.
2. Schreibe ein Python-Skript, das ‚ÄûHallo, KI!‚Äú an Ollama sendet und die Antwort ausgibt.

### ‚úÖ L√∂sungen

```sh
# L√∂sung 1: Installation
brew install ollama
ollama pull llama2
ollama serve
```

```python
# L√∂sung 2: Einfacher Prompt
import requests
prompt = "Hallo, KI!"
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": prompt})
print("Antwort:", r.json()["response"])
```
