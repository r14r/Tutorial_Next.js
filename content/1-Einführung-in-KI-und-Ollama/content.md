# ğŸ“˜ Tag 1 â€” EinfÃ¼hrung in KI & Ollama Grundlagen

## ğŸ“… Zeitplan 09:00 â€“ 15:00

### Ãœbersicht

Der Tag bietet einen praxisnahen Einstieg in KÃ¼nstliche Intelligenz und die Arbeit mit Ollama. Jede Einheit besteht aus einer kurzen ErklÃ¤rung, gefolgt von Beispielen und praktischen Ãœbungen.

---

### Zeitplan

| Zeit            | Einheit / Inhalt                                                                 | Dauer   |
|-----------------|----------------------------------------------------------------------------------|---------|
| **09:00 â€“ 09:20** | EinfÃ¼hrung & Ãœberblick Ã¼ber KI, ML, LLMs<br /><small>Motivation, Ziele des Tages</small> | 20 min  |
| **09:20 â€“ 09:50** | **Einheit 1.1:** Was ist KÃ¼nstliche Intelligenz?<br /><small>Begriff, Beispiele, Ãœbungen</small> | 30 min  |
| **09:50 â€“ 10:20** | **Einheit 1.2:** Einstieg in Ollama<br /><small>Installation, erster Test, Ãœbungen</small> | 30 min  |
| **10:20 â€“ 10:50** | **Einheit 1.3:** Ollama in der Kommandozeile<br /><small>CLI-Befehle, Dialog, Ãœbungen</small> | 30 min  |
| **10:50 â€“ 11:05** | â˜• **Pause**                                                                    | 15 min  |
| **11:05 â€“ 11:35** | **Einheit 1.4:** Ollama API mit cURL<br /><small>REST-API, Requests, Ãœbungen</small> | 30 min  |
| **11:35 â€“ 12:05** | **Einheit 1.5:** Zugriff Ã¼ber die API mit Python<br /><small>Requests, Chat, Ãœbungen</small> | 30 min  |
| **12:05 â€“ 12:35** | **Einheit 1.6:** Nutzung der Ollama SDKs<br /><small>Python SDK, Streaming, Ãœbungen</small> | 30 min  |
| **12:35 â€“ 13:35** | ğŸ½ï¸ **Mittagspause**                                                            | 60 min  |
| **13:35 â€“ 14:05** | **Einheit 1.7:** Grundlagen der Arbeit mit LLMs<br /><small>Funktionsweise, Zusammenfassung, Ãœbungen</small> | 30 min  |
| **14:05 â€“ 14:35** | **Einheit 1.8:** Prompt Engineering Grundlagen<br /><small>Prompts, Playground, Ãœbungen</small> | 30 min  |
| **14:35 â€“ 15:00** | **Zusammenfassung & Mini-Projekt**<br />Kleine KI-App mit Ollama und Streamlit | 25 min  |

---

## Ablauf der Einheiten

- **ErklÃ¤rung:** ca. 10â€“15 min
- **Beispiele:** ca. 10 min
- **Ãœbungen:** ca. 10â€“15 min  
    â†’ Ãœbungen werden direkt in Jupyter, VS Code oder Streamlit durchgefÃ¼hrt.

---

## Mini-Projekt (14:35â€“15:00)

Am Ende des Tages wird eine kleine KI-App entwickelt, die folgende Funktionen kombiniert:

- **Eingabe:** Textfeld fÃ¼r Prompts
- **Verarbeitung:** Anfrage an Ollama (lokales LLM)
- **Ausgabe:** Antwort anzeigen und ggf. zusammenfassen

---

## Einheit 1.1 â€” Was ist KÃ¼nstliche Intelligenz (KI)?

### ğŸ“– Hintergrund

KÃ¼nstliche Intelligenz (KI) ist der Bereich der Informatik, der Systeme entwickelt, die Aufgaben ausfÃ¼hren kÃ¶nnen, die normalerweise menschliche Intelligenz erfordern. Dazu gehÃ¶ren:

- ProblemlÃ¶sen (z. B. Schach spielen)
- Sprachverstehen (z. B. Chatbots)
- Mustererkennung (z. B. Gesichtserkennung)
- Entscheidungen treffen (z. B. Empfehlungssysteme)

Eine wichtige Unterdisziplin ist das Maschinelle Lernen (ML). Dabei lernen Systeme nicht durch feste Regeln, sondern aus Daten. Moderne KI basiert auf Neuronalen Netzen (Deep Learning). Besonders wichtig sind Large Language Models (LLMs) wie GPT, Llama oder Mistral, die auf groÃŸen Textmengen trainiert wurden und Sprache verarbeiten kÃ¶nnen.

### ğŸ’» Code-Beispiele

```python
# KI-Begriff einfach erklÃ¤rt
print("KI = Maschinen, die wie Menschen denken und handeln kÃ¶nnen.")
print("ML = Maschinen, die aus Daten lernen.")
```

### ğŸ“ Ãœbungen

1. Nenne drei reale Anwendungen von KI im Alltag.
2. ErklÃ¤re den Unterschied zwischen KI und ML.

### âœ… LÃ¶sungen

- Sprachassistenten (Siri, Alexa), Empfehlungssysteme (Netflix, Amazon), selbstfahrende Autos.
- KI = Ãœberbegriff; ML = Untergebiet, das Lernen aus Daten ermÃ¶glicht.

---

## Einheit 1.2 â€” Einstieg in Ollama

### ğŸ“– Hintergrund

Ollama ist eine Open-Source-Plattform, die es ermÃ¶glicht, groÃŸe Sprachmodelle lokal auszufÃ¼hren. Modelle kÃ¶nnen heruntergeladen werden (`ollama pull`). Man kann mit ihnen Ã¼ber eine API oder die Python-Bibliothek interagieren. Vorteil: Datenschutz (alles lÃ¤uft lokal), keine API-Kosten, offline nutzbar. Das macht Ollama ideal fÃ¼r schnelles Prototyping von KI-Apps.

### ğŸ’» Code-Beispiele

#### Streamlit-App: Chat mit Ollama

```python
import streamlit as st
import requests

st.title("Chat mit Ollama (lokales LLM)")

if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []

user_input = st.text_input("Du:", "")

if st.button("Senden") and user_input:
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "llama2", "prompt": user_input}
    )
    reply = response.json()["response"]
    st.session_state['chat_history'].append(("Du", user_input))
    st.session_state['chat_history'].append(("Ollama", reply))

for sender, msg in st.session_state['chat_history']:
    st.write(f"**{sender}:** {msg}")
```

#### API Health Check

```python
import requests

try:
    r = requests.get("http://localhost:11434")
    print("âœ… Ollama API lÃ¤uft!")
except Exception as e:
    print("âŒ Verbindung fehlgeschlagen:", e)
```

#### Installationshinweis

```sh
# macOS
brew install ollama

# Modelle laden
ollama pull llama2

# Server starten
ollama serve
```

### ğŸ“ Ãœbungen

1. Installiere Ollama lokal und lade das Modell llama2.
2. Schreibe ein Python-Skript, das â€Hallo, KI!â€œ an Ollama sendet und die Antwort ausgibt.

### âœ… LÃ¶sungen

```sh
# LÃ¶sung 1: Installation
brew install ollama
ollama pull llama2
ollama serve
```

```python
# LÃ¶sung 2: Einfacher Prompt
import requests
prompt = "Hallo, KI!"
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": prompt})
print("Antwort:", r.json()["response"])
```

---

## Einheit 1.3 â€” Ollama in der Kommandozeile (CLI)

### ğŸ“– Hintergrund

Ollama bringt ein eigenes Kommandozeilen-Tool mit. Damit kÃ¶nnen Modelle heruntergeladen, gelistet und direkt ausgefÃ¼hrt werden â€“ ohne Python.

- `ollama pull <modell>` â†’ Modell herunterladen
- `ollama list` â†’ verfÃ¼gbare Modelle anzeigen
- `ollama run <modell>` â†’ Modell starten und mit Prompts interagieren

### ğŸ’» Beispiele

```sh
# 1. Modell herunterladen
ollama pull llama2

# 2. Alle installierten Modelle anzeigen
ollama list

# 3. Direkt mit einem Modell chatten
ollama run llama2
```

#### Dialog in der CLI

```
>>> Was ist Python?
Python ist eine Programmiersprache, die fÃ¼r ihre Einfachheit und Vielseitigkeit bekannt ist.
```

#### Direkter Prompt von der Shell aus

```sh
echo "Schreibe ein Haiku Ã¼ber KI." | ollama run llama2
```

### ğŸ“ Ãœbungen

1. Lade das Modell mistral herunter.
2. Liste alle Modelle und Ã¼berprÃ¼fe, ob mistral installiert ist.
3. FÃ¼hre `ollama run mistral` aus und frage nach: â€Nenne mir drei Anwendungen von KI.â€œ

### âœ… LÃ¶sungen

```sh
ollama pull mistral
ollama list
ollama run mistral
```

Antwortbeispiel:

- Sprachassistenten
- Bildklassifikation
- Betrugserkennung

---

## Einheit 1.4 â€” Ollama API mit der Kommandozeile (cURL)

### ğŸ“– Hintergrund

Ollama lÃ¤uft standardmÃ¤ÃŸig auf Port 11434 und bietet eine REST-API. Mit `curl` lassen sich Requests direkt aus der Shell stellen â€“ ideal zum Testen.

### ğŸ’» Beispiele

```sh
# 1. Gesundheitscheck
curl http://localhost:11434

# 2. Einfacher Chat mit llama2
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Nenne drei Programmiersprachen"
}'
```

Antwort-Beispiel (JSON):

```json
{"response": "Python, Java, C++"}
```

#### Chat-API Beispiel

```sh
curl http://localhost:11434/api/chat -d '{
  "model": "llama2",
  "messages": [{"role": "user", "content": "ErklÃ¤re maschinelles Lernen in einfachen Worten."}]
}'
```

### ğŸ“ Ãœbungen

1. Nutze curl, um eine Liste aller Modelle von der API abzurufen (`/api/tags`).
2. Sende mit curl einen Prompt: â€Schreibe ein Gedicht Ã¼ber Programmierer.â€œ

### âœ… LÃ¶sungen

```sh
# 1. Modell-Liste
curl http://localhost:11434/api/tags

# 2. Gedicht
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Schreibe ein Gedicht Ã¼ber Programmierer."
}'
```

---

## Einheit 1.5 â€” Zugriff Ã¼ber die API mit Python (Requests)

### ğŸ“– Hintergrund

FÃ¼r KI-Apps nutzen wir meistens Python, um die API aufzurufen. Das geht mit der Bibliothek `requests`. Vorteil: JSON einlesen, Fehlerbehandlung mÃ¶glich, Integration in Streamlit.

### ğŸ’» Beispiele

```python
import requests

# Einfacher Prompt
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": "Was ist KI?"})
print(r.json()["response"])
```

#### Mehrere Nachrichten (Chat)

```python
messages = [
    {"role": "system", "content": "Du bist ein hilfreicher Assistent."},
    {"role": "user", "content": "ErklÃ¤re neuronale Netze kurz."}
]

r = requests.post("http://localhost:11434/api/chat",
                  json={"model": "llama2", "messages": messages})
print(r.json()["message"]["content"])
```

#### Streaming von Antworten

```python
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": "ZÃ¤hle von 1 bis 5"},
                  stream=True)

for line in r.iter_lines():
    if line:
        print(line.decode(), end="")
```

### ğŸ“ Ãœbungen

1. Schreibe ein Python-Skript, das eine Frage an Ollama stellt und die JSON-Antwort ausgibt.
2. Implementiere eine Funktion `chat(prompt)`, die eine Antwort vom Modell zurÃ¼ckgibt.

### âœ… LÃ¶sungen

```python
# 1. Einfaches Skript
import requests
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": "Nenne 5 LÃ¤nder in Europa"})
print(r.json())

# 2. Funktion
def chat(prompt):
    r = requests.post("http://localhost:11434/api/generate",
                      json={"model": "llama2", "prompt": prompt})
    return r.json()["response"]

print(chat("Was ist Python?"))
```

---

## Einheit 1.6 â€” Nutzung der Ollama SDKs

### ğŸ“– Hintergrund

Neben der API bietet Ollama SDKs (z. B. fÃ¼r Python oder Node.js). Diese vereinfachen die Kommunikation und bieten oft komfortablere Methoden als reine HTTP-Requests.

### ğŸ’» Beispiele

#### Python SDK

```python
import ollama

# Einfacher Chat
response = ollama.chat(
    model="llama2",
    messages=[{"role": "user", "content": "ErklÃ¤re KI in 2 SÃ¤tzen"}]
)
print(response["message"]["content"])
```

#### Mehrere Nachrichten

```python
history = [
    {"role": "system", "content": "Du bist ein Tutor."},
    {"role": "user", "content": "Was ist Maschinelles Lernen?"}
]

response = ollama.chat(model="llama2", messages=history)
print(response["message"]["content"])
```

#### Liste der Modelle

```python
response = ollama.list()
for m in response.models:
    print("Modell:", m["model"])
```

#### Streaming im SDK

```python
response = ollama.chat(
    model="llama2",
    messages=[{"role": "user", "content": "ZÃ¤hle von 1 bis 5"}],
    stream=True
)

for chunk in response:
    print(chunk["message"]["content"], end="", flush=True)
```

### ğŸ“ Ãœbungen

1. Nutze das SDK, um eine Liste aller installierten Modelle auszugeben.
2. Schreibe ein kleines Chat-Skript, das eine Unterhaltung in einer Schleife ermÃ¶glicht.

### âœ… LÃ¶sungen

```python
# 1. Liste der Modelle
import ollama
response = ollama.list()
print([m["model"] for m in response.models])

# 2. Einfacher CLI-Chat
while True:
    user = input("Du: ")
    if user.lower() == "quit":
        break
    response = ollama.chat(model="llama2",
                           messages=[{"role": "user", "content": user}])
    print("Ollama:", response["message"]["content"])
```

---

## Einheit 1.7 â€” Grundlagen der Arbeit mit LLMs

### ğŸ“– Hintergrund

Large Language Models (LLMs) sind KI-Modelle, die auf Milliarden von Texten trainiert wurden. Sie lernen, das nÃ¤chste Wort in einem Satz vorherzusagen. Dadurch kÃ¶nnen sie Texte generieren, Fragen beantworten, zusammenfassen. Typische Modelle: GPT-4, Llama-2, Mistral. Ollama ermÃ¶glicht, diese Modelle lokal zu nutzen. Einsatz in KI-Apps: Chatbots, Textzusammenfassungen, Code-Helfer.

### ğŸ’» Code-Beispiele

#### Text-Zusammenfasser mit Ollama

```python
import streamlit as st
import requests

st.title("Dokument-Zusammenfassung mit Ollama")
file = st.file_uploader("Textdatei hochladen", type=["txt"])

if file:
    text = file.read().decode()
    prompt = f"Fasse den folgenden Text in 5 SÃ¤tzen zusammen:\n\n{text}"
    response = requests.post("http://localhost:11434/api/generate",
                             json={"model": "llama2", "prompt": prompt})
    summary = response.json()["response"]
    st.subheader("Zusammenfassung")
    st.write(summary)
```

### ğŸ“ Ãœbungen

1. ErklÃ¤re, warum LLMs nicht immer richtige Antworten geben (Halluzinationen).
2. Schreibe ein Skript, das eine Frage an Ollama stellt und die Antwort ausgibt.

### âœ… LÃ¶sungen

- LLMs basieren auf Wahrscheinlichkeiten und erfinden manchmal plausible, aber falsche Inhalte â†’ â€Halluzinationenâ€œ.

```python
import requests
frage = "Was ist die Hauptstadt von Frankreich?"
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": frage})
print("Antwort:", r.json()["response"])
```

---

## Einheit 1.8 â€” Prompt Engineering Grundlagen

### ğŸ“– Hintergrund

Prompt Engineering = die Kunst, gute Eingaben fÃ¼r ein Sprachmodell zu formulieren. Unterschiedliche Formulierungen fÃ¼hren zu anderen Ergebnissen. Ein guter Prompt enthÃ¤lt:

- klare Anweisung
- Kontext
- gewÃ¼nschtes Format

Beispiel: â€Fasse den Text zusammenâ€œ vs. â€Fasse den Text in 3 Bullet Points zusammenâ€œ.

### ğŸ’» Code-Beispiele

#### Prompt Playground in Streamlit

```python
import streamlit as st
import requests

st.title("Prompt Playground")
prompt = st.text_area("Prompt eingeben", height=150)

if st.button("Abschicken") and prompt:
    response = requests.post("http://localhost:11434/api/generate",
                             json={"model": "llama2", "prompt": prompt})
    st.subheader("Antwort")
    st.write(response.json()["response"])
```

### ğŸ“ Ãœbungen

1. Erstelle drei verschiedene Prompts, die denselben Text zusammenfassen, aber mit unterschiedlichem Stil (kurz, ausfÃ¼hrlich, Bulletpoints).
2. Teste mit Ollama, wie sich die Ergebnisse unterscheiden.

### âœ… LÃ¶sungen

- Prompt 1: â€Fasse diesen Artikel in 1 Satz zusammen.â€œ
- Prompt 2: â€Schreibe eine detaillierte Zusammenfassung in 5 SÃ¤tzen.â€œ
- Prompt 3: â€Fasse den Artikel in Bulletpoints zusammen.â€œ

---

## ğŸ¯ Zusammenfassung Tag 1 (erweitert)

Nach diesem Tag kÃ¶nnen die Studierenden:

- erklÃ¤ren, was KI, ML und LLMs sind,
- Ollama lokal installieren und erste Modelle nutzen,
- Ollama Ã¼ber die CLI bedienen (`pull`, `list`, `run`),
- Ollama Ã¼ber die REST-API mit `curl` ansprechen,
- Ollama in Python-Apps mit `requests` einbinden,
- Ollama komfortabel Ã¼ber das SDK nutzen (inkl. Streaming),
- einfache KI-Apps mit Streamlit und Ollama bauen,
- Prompts gezielt formulieren, um bessere Ergebnisse zu erhalten.
