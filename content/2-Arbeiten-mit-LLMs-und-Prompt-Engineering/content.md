# Tag 2 ‚Äî Arbeiten mit LLMs & Prompt Engineering

## üìÖ Zeitplan 09:00 ‚Äì 15:00

### √úbersicht

Der Tag bietet eine praxisnahe Einf√ºhrung in Large Language Models (LLMs) und Prompt Engineering. Jede kombiniert kurze Erkl√§rungen, Code-Beispiele und praktische √úbungen.

---

### Zeitplan

| Zeit              | / Inhalt                                                                                   | Dauer   |
|-------------------|----------------------------------------------------------------------------------------------------|---------|
| **09:00 ‚Äì 09:20** | Einf√ºhrung & √úberblick,   Was sind LLMs? Ziele des Tages                                           | 20 min  |
| **09:20 ‚Äì 09:50** | **2.1:** Einf√ºhrung in LLMs,   Hintergrund, Einsatzgebiete, Grenzen                        | 30 min  |
| **09:50 ‚Äì 10:20** | **2.2:** Was ist ein Prompt? Prompt-Elemente, Beispiele, √úbungen                           | 30 min  |
| **10:20 ‚Äì 10:50** | **2.3:** Rollen im Prompting & Grundlagen,   System/User/Assistant, Prompt Engineering                                         | 30 min  |
| **10:50 ‚Äì 11:05** | ‚òï **Pause**                                                                                        | 15 min  |
| **11:05 ‚Äì 11:35** | **2.4:** Prompting-Prinzipien ,   Zero-/Few-Shot, Chain-of-Thought, Output-Format                                   | 30 min  |
| **11:35 ‚Äì 12:05** | **2.5:** Prompt-Playground & Vergleich,   Streamlit-App, Prompt-Experimente                                                 | 30 min  |
| **12:05 ‚Äì 13:05** | üçΩÔ∏è **Mittagspause**                                                                                | 60 min  |
| **13:05 ‚Äì 13:35** | **2.6:** Temperatur & Kreativit√§t ,  Parameter, Temperature, Max Tokens                                                | 30 min  |
| **13:35 ‚Äì 14:05** | **2.7:** Vergleich verschiedener Modelle ,   llama2, mistral, codellama                                                        | 30 min  |
| **14:05 ‚Äì 14:35** | **2.8:** Fehler & Grenzen von LLMs  ,   Halluzinationen, Bias, Kontext                                                    | 30 min  |
| **14:35 ‚Äì 14:55** | **2.9:** Praktische Prompt-Beispiele  ,   Zusammenfassung, Rollen, Code, JSON                                               | 20 min  |
| **14:55 ‚Äì 15:00** | **Zusammenfassung & Ausblick**  ,   Wiederholung, Fragen, n√§chste Schritte                                            | 5 min   |

---

## Ablauf der en

- **Erkl√§rung:** ca. 10‚Äì15 min
- **Code-Beispiele:** ca. 10 min
- **√úbungen:** ca. 10‚Äì15 min  
    ‚Üí √úbungen werden direkt im Editor, Jupyter oder Streamlit durchgef√ºhrt.

---

## 2.1 ‚Äî Einf√ºhrung in Large Language Models (LLMs)

### Hintergrund

Large Language Models (LLMs) wie GPT, Llama, Mistral oder Gemma sind KI-Modelle, die auf Milliarden von Texten trainiert wurden.  
Sie lernen, das n√§chste Wort vorherzusagen und entwickeln dadurch erstaunliche Sprachf√§higkeiten.

**Einsatzgebiete:**  

- Textgenerierung  
- Zusammenfassung  
- √úbersetzung  
- Q&A  
- Codegenerierung

LLMs sind probabilistisch (nicht deterministisch), d. h. bei gleichem Prompt kann es verschiedene Antworten geben.

**Grenzen:**  

- Halluzinationen (falsche Antworten)  
- Kein echtes ‚ÄûVerstehen‚Äú, sondern Mustererkennung  
- Kontextlimitierungen

---

## 2.2 ‚Äî Was ist ein Prompt?

### Hintergrund

Ein Prompt ist die Eingabe, die ein Mensch einem Sprachmodell (LLM) gibt.  
Man kann sich ein Prompt vorstellen wie eine Anweisung an ein sehr intelligentes, aber textbasiertes Programm.

**Elemente eines guten Prompts:**  

- Rolle ‚Äì definiert, in welcher ‚ÄûRolle‚Äú das Modell antworten soll (z. B. ‚ÄûDu bist ein Lehrer‚Ä¶‚Äú)
- Kontext ‚Äì liefert Hintergrundinformationen oder Daten, auf die sich die Antwort beziehen soll
- Aufgabe ‚Äì die eigentliche Anweisung (‚ÄûFasse den Text zusammen‚Äú, ‚ÄûSchreibe ein Gedicht‚Äú)
- Format ‚Äì gew√ºnschte Ausgabeform (z. B. Bulletpoints, JSON, Tabellenform)

### Code-Beispiele

```python
import requests

prompt = "Erkl√§re Maschinelles Lernen in einfachen Worten."
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": prompt})
print(r.json()["response"])
```

**Prompt mit Rolle & Format:**

```python
prompt = """Du bist ein Universit√§tsprofessor f√ºr Informatik.
Erkl√§re den Begriff 'Maschinelles Lernen' in 3 Bulletpoints f√ºr Studierende im ersten Semester."""
```

**Prompt mit Kontext:**

```python
text = "Python ist eine Programmiersprache, die 1991 von Guido van Rossum entwickelt wurde."
prompt = f"Fasse den folgenden Text in 2 S√§tzen zusammen:\n\n{text}"
```

### √úbungen

1. Nenne drei typische Anwendungen von LLMs.
2. Erkl√§re, warum LLMs manchmal halluzinieren.

### L√∂sungen

- Chatbots, Textzusammenfassungen, Programmierhilfe.
- Sie basieren auf Wahrscheinlichkeiten, nicht auf ‚ÄûWissen‚Äú.

---

## 2.3 ‚Äî Rollen im Prompting & Prompt Engineering: Grundlagen

### Hintergrund

Prompt Engineering bezeichnet die Kunst, Eingaben (Prompts) so zu formulieren, dass das Modell bestm√∂gliche Ergebnisse liefert.

In modernen APIs (z. B. Ollama, OpenAI) bestehen Prompts aus Nachrichten mit Rollen:

- **system** ‚Äì definiert die Identit√§t des Modells (‚ÄûDu bist ein hilfreicher Tutor‚Ä¶‚Äú)
- **user** ‚Äì enth√§lt die Anfrage des Nutzers
- **assistant** ‚Äì (optional) vorherige Antworten des Modells, um Konversationen fortzuf√ºhren
- **tool/function** ‚Äì (in erweiterten Frameworks) spezielle Funktionsaufrufe

Ein Prompt enth√§lt:

- Aufgabe (‚ÄûFasse zusammen‚Ä¶‚Äú)
- Kontext (‚ÄûDer Text ist ein Artikel √ºber‚Ä¶‚Äú)
- Format (‚Äûin Bulletpoints‚Äú)

Unterschiedliche Formulierungen ‚Üí unterschiedliche Ergebnisse.

### Code-Beispiele

**Prompt mit Rollen:**

```python
messages = [
    {"role": "system", "content": "Du bist ein freundlicher Lehrer."},
    {"role": "user", "content": "Erkl√§re neuronale Netze in einfachen Worten."}
]

r = requests.post("http://localhost:11434/api/chat",
                  json={"model": "llama2", "messages": messages})
print(r.json()["message"]["content"])
```

**Vergleich zweier Prompts:**

```python
p1 = "Fasse den Text kurz zusammen."
p2 = "Erstelle eine ausf√ºhrliche Zusammenfassung in 5 S√§tzen."

for p in [p1, p2]:
    r = requests.post("http://localhost:11434/api/generate",
                      json={"model": "llama2", "prompt": p})
    print("Prompt:", p)
    print("Antwort:", r.json()["response"])
```

### √úbungen

1. Formuliere drei Prompts, die denselben Text unterschiedlich zusammenfassen.
2. Experimentiere: Welche Formulierung liefert die besten Ergebnisse?

### L√∂sungen

- Prompt 1: ‚ÄûFasse in 1 Satz zusammen.‚Äú
- Prompt 2: ‚ÄûFasse in 5 S√§tzen zusammen.‚Äú
- Prompt 3: ‚ÄûFasse in Bulletpoints zusammen.‚Äú

---

## 2.4 ‚Äî Prompting-Prinzipien

### Hintergrund

Um bessere Ergebnisse zu erzielen, gibt es verschiedene Strategien:

- **Zero-Shot Prompting** ‚Äì Aufgabe ohne Beispiele
- **Few-Shot Prompting** ‚Äì Aufgabe mit Beispielen
- **Role Prompting** ‚Äì Modell in eine Rolle versetzen
- **Chain-of-Thought (CoT)** ‚Äì Modell auffordern, Schritt-f√ºr-Schritt zu denken
- **Self-Consistency** ‚Äì mehrere Antworten generieren und vergleichen
- **Output-Format Prompting** ‚Äì Antwort in bestimmtem Format anfordern

### Beispiele

**Zero-Shot Prompting:**

```python
prompt = "√úbersetze folgenden Satz ins Franz√∂sische: 'Ich liebe KI.'"
```

**Few-Shot Prompting:**

```python
prompt = """√úbersetze ins Franz√∂sische:

Deutsch: Hallo ‚Üí Franz√∂sisch: Bonjour
Deutsch: Danke ‚Üí Franz√∂sisch: Merci
Deutsch: Ich liebe KI ‚Üí Franz√∂sisch:"""
```

**Role Prompting:**

```python
prompt = """Du bist ein Arzt.
Erkl√§re die Symptome einer Grippe f√ºr einen Patienten in einfacher Sprache."""
```

**Chain-of-Thought Prompting:**

```python
prompt = """L√∂se die Aufgabe Schritt f√ºr Schritt:
Frage: Ein Apfel kostet 2‚Ç¨, eine Orange 3‚Ç¨. Wie viel kostet es, 4 √Ñpfel und 2 Orangen zu kaufen?"""
```

**Self-Consistency:**

```python
for i in range(3):
    r = requests.post("http://localhost:11434/api/generate",
                      json={"model": "llama2", "prompt": "Rechne 37+45 Schritt f√ºr Schritt."})
    print(r.json()["response"])
```

**Output-Format Prompting (JSON):**

```python
prompt = """Antworte nur im folgenden JSON-Format:
{
  "Name": "<string>",
  "Alter": <int>
}
Frage: Person hei√üt Alice und ist 30 Jahre alt."""
```

---

## 2.5 ‚Äî Prompt-Playground & Prompt-Vergleich (Streamlit-App)

### Hintergrund

Ein Prompt-Playground ist eine kleine App, in der man Prompts eingeben und sofort sehen kann, wie das Modell reagiert.  
Hilfreich zum Experimentieren.  
Ideal, um zu verstehen, wie kleine √Ñnderungen den Output beeinflussen.

Man kann eine App bauen, die zwei Prompts nebeneinander testet ‚Äì sehr n√ºtzlich zum Prompt Engineering.

### Code-Beispiele

**Prompt Playground:**

```python
import streamlit as st
import requests

st.title("Prompt Playground")
model = st.selectbox("Modell w√§hlen", ["llama2", "mistral"])
prompt = st.text_area("Prompt eingeben")

if st.button("Start") and prompt:
    r = requests.post("http://localhost:11434/api/generate",
                      json={"model": model, "prompt": prompt})
    st.markdown(r.json()["response"])
```

**Prompt-Vergleich:**

```python
import streamlit as st
import requests

st.title("Prompt-Vergleich")

p1 = st.text_area("Prompt 1")
p2 = st.text_area("Prompt 2")
model = st.selectbox("Modell", ["llama2", "mistral"])

if st.button("Vergleichen"):
    for i, p in enumerate([p1, p2], start=1):
        if p:
            r = requests.post("http://localhost:11434/api/generate",
                              json={"model": model, "prompt": p})
            st.subheader(f"Antwort {i}")
            st.write(r.json()["response"])
```

### √úbungen

1. Erweitere den Playground um ein Dropdown-Men√º zur Auswahl verschiedener Modelle.
2. F√ºge eine Option hinzu, die die Antwort in Markdown darstellt.

---

## 2.6 ‚Äî Temperatur & Steuerung der Kreativit√§t

### Hintergrund

LLMs haben Parameter, die das Verhalten beeinflussen:

- **Temperature:** 0 = deterministisch, 1 = kreativ, >1 = chaotisch
- **Max Tokens:** begrenzt die L√§nge der Antwort
- **Top-k / Top-p:** steuern die Auswahlwahrscheinlichkeit von W√∂rtern

### Code-Beispiel

```python
prompt = "Schreibe eine kurze Geschichte √ºber eine Katze und einen Roboter."

for t in [0.0, 0.7, 1.2]:
    r = requests.post("http://localhost:11434/api/generate",
                      json={"model": "llama2", "prompt": prompt,
                            "options": {"temperature": t}})
    print(f"Temp={t}:", r.json()["response"])
```

### √úbungen

1. Erzeuge denselben Prompt mit temperature = 0.0, 0.7 und 1.5. Vergleiche die Ergebnisse.
2. Was ist besser f√ºr technische Dokumentationen ‚Äì hohe oder niedrige Temperatur?

### L√∂sungen

- Unterschied: 0.0 = sehr n√ºchtern, 0.7 = kreativ, 1.5 = chaotisch.
- Niedrige Temperatur (pr√§ziser, weniger kreative Ausschweifungen).

---

## 2.7 ‚Äî Vergleich verschiedener Modelle

### Hintergrund

Ollama unterst√ºtzt mehrere Modelle. Jedes hat St√§rken & Schw√§chen:

- **Llama2:** allgemeines Sprachmodell
- **Mistral:** schnell & kompakt
- **Codellama:** spezialisiert auf Programmierung

### Code-Beispiel

```python
import requests

models = ["llama2", "mistral"]
prompt = "Erkl√§re den Unterschied zwischen Python und Java."

for m in models:
    r = requests.post("http://localhost:11434/api/generate",
                      json={"model": m, "prompt": prompt})
    print(m, ":", r.json()["response"])
```

### √úbungen

1. Vergleiche llama2 und mistral bei der Aufgabe: ‚ÄûFasse einen Text in 3 S√§tzen zusammen.‚Äú
2. Teste ein Code-Modell mit dem Prompt: ‚ÄûSchreibe eine Python-Funktion f√ºr die Fibonacci-Zahlen.‚Äú

### L√∂sungen

- Modelle geben unterschiedliche Stile zur√ºck ‚Äì mistral oft k√ºrzer, llama2 detaillierter.
- Codellama liefert meist besseren Python-Code als ein Standardmodell.

---

## 2.8 ‚Äî Fehler & Grenzen von LLMs

### Hintergrund

LLMs sind m√§chtig, aber nicht perfekt:

- Halluzinationen: erfundene Fakten
- Bias: Vorurteile aus Trainingsdaten
- Keine Rechenmaschine: Mathe kann fehlerhaft sein
- Kontextfenster: zu lange Texte werden abgeschnitten

### Code-Beispiel

```python
prompt = "Was ist die Hauptstadt von Frankreich in Afrika?"
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": prompt})
print(r.json()["response"])
# ‚Üí Beispiel f√ºr Halluzination
```

### √úbungen

1. Probiere absichtlich fehlerhafte Fragen (z. B. ‚ÄûWer hat 1800 den Nobelpreis f√ºr KI gewonnen?‚Äú).
2. Analysiere, warum die Antworten trotzdem plausibel klingen.

### L√∂sungen

- Das Modell ‚Äûhalluziniert‚Äú eine Antwort.
- Es wurde darauf trainiert, wahrscheinlich klingende Texte zu erzeugen, nicht Fakten.

---

## 2.9 ‚Äî Praktische Beispiele f√ºr Prompts

1. Text zusammenfassen  
   `prompt = "Fasse folgenden Text in 3 Bulletpoints zusammen:\nText: ..."`

2. Als Rolle agieren  
   `prompt = "Du bist ein Reiseberater. Empfiehl mir 3 Orte in Italien."`

3. Korrekturlesen  
   `prompt = "Korrigiere Rechtschreibfehler im folgenden Text: 'Das is ein Beispil'."`

4. Code erkl√§ren  
   `prompt = "Erkl√§re diesen Python-Code Schritt f√ºr Schritt:\n\nfor i in range(5): print(i)"`

5. JSON-Ausgabe erzwingen  

   ```python
   prompt = """Analysiere folgenden Satz: 'Die KI ist faszinierend.'
   Antworte im JSON-Format:
   {"Sprache": "Deutsch", "Stimmung": "positiv"}"""
   ```

---

## Zusammenfassung Tag 2

Nach Tag 2 k√∂nnen die Studierenden:

- erkl√§ren, wie LLMs arbeiten und wo ihre Grenzen liegen,
- Prompts gezielt formulieren und die Antworten vergleichen,
- die Parameter (Temperature, Tokens) steuern,
- mit mehreren Modellen experimentieren,
- einen Prompt-Playground in Streamlit bauen,
- verstehen, was ein Prompt ist und wie er aufgebaut sein sollte (Rolle, Kontext, Aufgabe, Format),
- die verschiedenen Rollen (system, user, assistant) einsetzen,
- die wichtigsten Prompt-Prinzipien anwenden: Zero-/Few-Shot, Role, Chain-of-Thought, Self-Consistency, Format-Prompting,
- Prompts f√ºr konkrete Aufgaben wie Zusammenfassung, Rollen, Code-Erkl√§rung oder JSON-Ausgaben formulieren.
