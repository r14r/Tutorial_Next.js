## Einheit 2 ‚Äî Einstieg in Ollama

### üìñ Hintergrund

Ollama ist eine Open-Source-Plattform, die es erm√∂glicht, gro√üe Sprachmodelle lokal auszuf√ºhren. Modelle k√∂nnen heruntergeladen werden (`ollama pull`). Man kann mit ihnen √ºber eine API oder die Python-Bibliothek interagieren. Vorteil: Datenschutz (alles l√§uft lokal), keine API-Kosten, offline nutzbar. Das macht Ollama ideal f√ºr schnelles Prototyping von KI-Apps.

### üíª Code-Beispiele

#### Streamlit-App: Chat mit Ollama (mit ollama)

```python
import streamlit as st
from lib.helper_ollama import ollama

st.title("Chat mit Ollama (lokales LLM)")

if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []

user_input = st.text_input("Du:", "")

if st.button("Senden") and user_input:
    reply = ollama.generate(user_input)
    st.session_state['chat_history'].append(("Du", user_input))
    st.session_state['chat_history'].append(("Ollama", reply))

for sender, msg in st.session_state['chat_history']:
    st.write(f"**{sender}:** {msg}")
```

#### API Health Check mit ollama

```python
from lib.helper_ollama import ollama

try:
    result = ollama.generate("Test")
    print("‚úÖ Ollama API l√§uft!", result[:50])
except Exception as e:
    print("‚ùå Verbindung fehlgeschlagen:", e)
```

#### Installationshinweis

```sh
# macOS
brew install ollama

# Modelle laden
ollama pull llama2

# Server starten
ollama serve
```

### üìù √úbungen

1. Installiere Ollama lokal und lade das Modell llama2.
2. Schreibe ein Python-Skript, das ‚ÄûHallo, KI!‚Äú an Ollama sendet und die Antwort ausgibt.

### ‚úÖ L√∂sungen

```sh
# L√∂sung 1: Installation
brew install ollama
ollama pull llama2
ollama serve
```

```python
# L√∂sung 2: Einfacher Prompt
import requests
prompt = "Hallo, KI!"
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": prompt})
print("Antwort:", r.json()["response"])
```

---

