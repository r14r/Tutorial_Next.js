## Einheit 5 ‚Äî Zugriff √ºber die API mit Python (Requests)

### üìñ Hintergrund

F√ºr KI-Apps nutzen wir meistens Python, um die API aufzurufen. Das geht mit der Bibliothek `requests`. Vorteil: JSON einlesen, Fehlerbehandlung m√∂glich, Integration in Streamlit.

### üíª Beispiele

```python
from lib.helper_ollama import ollama

# Einfacher Prompt mit ollama
result = ollama.generate("Was ist KI?")
print(result)
```

#### Mehrere Nachrichten (Chat) mit ollama

```python
from lib.helper_ollama import ollama

messages = [
    {"role": "system", "content": "Du bist ein hilfreicher Assistent."},
    {"role": "user", "content": "Erkl√§re neuronale Netze kurz."}
]

result = ollama.chat(messages)
print(result)
```

#### Streaming von Antworten

```python
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": "Z√§hle von 1 bis 5"},
                  stream=True)

for line in r.iter_lines():
    if line:
        print(line.decode(), end="")
```

### üìù √úbungen

1. Schreibe ein Python-Skript, das eine Frage an Ollama stellt und die JSON-Antwort ausgibt.
2. Implementiere eine Funktion `chat(prompt)`, die eine Antwort vom Modell zur√ºckgibt.

### ‚úÖ L√∂sungen

```python
# 1. Einfaches Skript
import requests
r = requests.post("http://localhost:11434/api/generate",
                  json={"model": "llama2", "prompt": "Nenne 5 L√§nder in Europa"})
print(r.json())

# 2. Funktion
def chat(prompt):
    r = requests.post("http://localhost:11434/api/generate",
                      json={"model": "llama2", "prompt": prompt})
    return r.json()["response"]

print(chat("Was ist Python?"))
```

---

