## Einheit 12 — Zusammenfassung

Nach Tag 2 können die Studierenden:

- erklären, wie LLMs arbeiten und wo ihre Grenzen liegen,
- Prompts gezielt formulieren und die Antworten vergleichen,
- die Parameter (Temperature, Tokens) steuern,
- mit mehreren Modellen experimentieren,
- einen Prompt-Playground in Streamlit bauen,
- verstehen, was ein Prompt ist und wie er aufgebaut sein sollte (Rolle, Kontext, Aufgabe, Format),
- die verschiedenen Rollen (system, user, assistant) einsetzen,
- die wichtigsten Prompt-Prinzipien anwenden: Zero-/Few-Shot, Role, Chain-of-Thought, Self-Consistency, Format-Prompting,
- Prompts für konkrete Aufgaben wie Zusammenfassung, Rollen, Code-Erklärung oder JSON-Ausgaben formulieren.

---

