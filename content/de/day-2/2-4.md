## Einheit 4 — Prompting-Prinzipien

### Umfangreiche Beschreibung & Grundlagen

Um die besten Ergebnisse aus LLMs herauszuholen, gibt es verschiedene Prompting-Prinzipien und Strategien. Sie helfen, die Antworten gezielt zu steuern, Fehler zu vermeiden und die Kreativität des Modells zu nutzen. Die wichtigsten Prinzipien sind:

- **Zero-Shot Prompting:** Das Modell erhält nur die Aufgabe, aber keine Beispiele. Es muss die Aufgabe „aus dem Stand“ lösen.
- **Few-Shot Prompting:** Der Prompt enthält zusätzlich Beispiele, wie die Aufgabe gelöst werden soll. Das Modell kann sich daran orientieren.
- **Role Prompting:** Das Modell wird in eine bestimmte Rolle versetzt (z. B. „Du bist ein Arzt…“), um die Antwort zu beeinflussen.
- **Chain-of-Thought (CoT):** Das Modell wird aufgefordert, Schritt für Schritt zu denken und zu argumentieren.
- **Self-Consistency:** Mehrere Antworten werden generiert, um die beste oder konsistenteste auszuwählen.
- **Output-Format Prompting:** Das gewünschte Ausgabeformat wird explizit vorgegeben (z. B. JSON, Tabelle, Bulletpoints).

**Warum sind diese Prinzipien wichtig?**

- Sie erhöhen die Zuverlässigkeit und Nachvollziehbarkeit der Antworten.
- Sie helfen, Fehler und Missverständnisse zu vermeiden.
- Sie ermöglichen strukturierte, formatierte und kreative Ergebnisse.

### 5 Beispiele für Prompting-Prinzipien (mit Python-Code)

1. **Zero-Shot Prompting:**

    Prompt:

    ```prompt
        "Übersetze folgenden Satz ins Französische: 'Ich liebe KI.'"
    ```

    ```python
    # Datei: prinzip1_zeroshot.py
    from lib.helper_ollama import ollama
    prompt = "Übersetze folgenden Satz ins Französische: 'Ich liebe KI.'"
    result = ollama.generate(prompt)
    print(result)
    ```

2. **Few-Shot Prompting:**

    Prompt:

    ```prompt
        "Übersetze ins Französische:\nDeutsch: Hallo → Französisch: Bonjour\nDeutsch: Danke → Französisch: Merci\nDeutsch: Ich liebe KI → Französisch:"
    ```

    ```python
    # Datei: prinzip2_fewshot.py
    from lib.helper_ollama import ollama
    prompt = "Übersetze ins Französische:\nDeutsch: Hallo → Französisch: Bonjour\nDeutsch: Danke → Französisch: Merci\nDeutsch: Ich liebe KI → Französisch:"
    result = ollama.generate(prompt)
    print(result)
    ```

3. **Role Prompting:**

    Prompt:

    ```prompt
        "Du bist ein Arzt. Erkläre die Symptome einer Grippe für einen Patienten in einfacher Sprache."
    ```

    ```python
    # Datei: prinzip3_role.py
    from lib.helper_ollama import ollama
    prompt = "Du bist ein Arzt. Erkläre die Symptome einer Grippe für einen Patienten in einfacher Sprache."
    result = ollama.generate(prompt)
    print(result)
    ```

4. **Chain-of-Thought Prompting:**

    Prompt:

    ```prompt
        "Löse die Aufgabe Schritt für Schritt: Ein Apfel kostet 2€, eine Orange 3€. Wie viel kostet es, 4 Äpfel und 2 Orangen zu kaufen?"
    ```

    ```python
    # Datei: prinzip4_chainofthought.py
    from lib.helper_ollama import ollama
    prompt = "Löse die Aufgabe Schritt für Schritt: Ein Apfel kostet 2€, eine Orange 3€. Wie viel kostet es, 4 Äpfel und 2 Orangen zu kaufen?"
    result = ollama.generate(prompt)
    print(result)
    ```

5. **Output-Format Prompting:**

    Prompt:

    ```prompt
    "Antworte im JSON-Format: {\"Name\": \"<string>\", \"Alter\": <int>} Frage: Person heißt Alice und ist 30 Jahre alt."
    ```

    ```python
    # Datei: prinzip5_format.py
    from lib.helper_ollama import ollama
    prompt = "Antworte im JSON-Format: {\"Name\": \"<string>\", \"Alter\": <int>} Frage: Person heißt Alice und ist 30 Jahre alt."
    result = ollama.generate(prompt)
    print(result)
    ```

### 10 Fragen & Lösungen zu Prompting-Prinzipien

1. **Was ist Zero-Shot Prompting?**

*Lösung:* Das Modell erhält nur die Aufgabe, aber keine Beispiele.
2. **Wie hilft Few-Shot Prompting?**

*Lösung:* Durch Beispiele kann das Modell die Aufgabe besser verstehen und lösen.
3. **Was ist Role Prompting?**

*Lösung:* Das Modell wird in eine bestimmte Rolle versetzt, um die Antwort zu beeinflussen.
4. **Wozu dient Chain-of-Thought Prompting?**

*Lösung:* Das Modell wird aufgefordert, Schritt für Schritt zu denken und zu argumentieren.
5. **Was ist Self-Consistency?**

*Lösung:* Mehrere Antworten werden generiert, um die beste auszuwählen.
6. **Wie kann man das Ausgabeformat steuern?**

*Lösung:* Durch explizite Vorgabe im Prompt (z. B. „im JSON-Format“).
7. **Warum sind Beispiele im Prompt hilfreich?**

*Lösung:* Sie zeigen dem Modell, wie die Aufgabe gelöst werden soll.
8. **Was ist ein Nachteil von Zero-Shot Prompting?**

*Lösung:* Die Antwort kann ungenau oder unerwartet sein, da keine Beispiele vorliegen.
9. **Wie kann man die Kreativität des Modells fördern?**

*Lösung:* Durch offene Aufgabenstellungen und kreative Rollen.
10. **Was ist ein typischer Fehler bei Prompting-Prinzipien?**

*Lösung:* Fehlende Beispiele, unklare Rollen oder kein Format vorgegeben.

---

st.title("Prompt Playground")
st.title("Prompt-Vergleich")
p1 = st.text_area("Prompt 1")
p2 = st.text_area("Prompt 2")

