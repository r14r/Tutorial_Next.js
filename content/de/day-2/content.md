# Tag 2 ‚Äî Arbeiten mit LLMs & Prompt Engineering

## Einheit 1 ‚Äî Zeitplan 09:00 ‚Äì 15:00

### √úbersicht

Der Tag bietet eine praxisnahe Einf√ºhrung in Large Language Models (LLMs) und Prompt Engineering. Jede Einheit kombiniert kurze Erkl√§rungen, Code-Beispiele und praktische √úbungen.

---

### Zeitplan

| Zeit            | Einheit / Inhalt                                                                 | Dauer   |
|-----------------|----------------------------------------------------------------------------------|---------|
| **09:00 ‚Äì 09:20** | Einf√ºhrung & √úberblick - Was sind LLMs? Ziele des Tages         | 20 min  |
| **09:20 ‚Äì 09:50** | **Einheit 2.1:** Einf√ºhrung in LLMs - Hintergrund, Einsatzgebiete, Grenzen | 30 min  |
| **09:50 ‚Äì 10:20** | **Einheit 2.2:** Was ist ein Prompt? - Prompt-Elemente, Beispiele, √úbungen | 30 min  |
| **10:20 ‚Äì 10:50** | **Einheit 2.3:** Rollen im Prompting & Grundlagen - System/User/Assistant, Prompt Engineering | 30 min  |
| **10:50 ‚Äì 11:05** | ‚òï **Pause**                                                                    | 15 min  |
| **11:05 ‚Äì 11:35** | **Einheit 2.4:** Prompting-Prinzipien - Zero-/Few-Shot, Chain-of-Thought, Output-Format | 30 min  |
| **11:35 ‚Äì 12:05** | **Einheit 2.5:** Prompt-Playground & Vergleich - Streamlit-App, Prompt-Experimente | 30 min  |
| **12:05 ‚Äì 13:05** | üçΩÔ∏è **Mittagspause**                                                            | 60 min  |
| **13:05 ‚Äì 13:35** | **Einheit 2.6:** Temperatur & Kreativit√§t - Parameter, Temperature, Max Tokens | 30 min  |
| **13:35 ‚Äì 14:05** | **Einheit 2.7:** Vergleich verschiedener Modelle - llama2, mistral, codellama | 30 min  |
| **14:05 ‚Äì 14:35** | **Einheit 2.8:** Fehler & Grenzen von LLMs - Halluzinationen, Bias, Kontext | 30 min  |
| **14:35 ‚Äì 14:55** | **Einheit 2.9:** Praktische Prompt-Beispiele - Zusammenfassung, Rollen, Code, JSON | 20 min  |
| **14:55 ‚Äì 15:00** | **Zusammenfassung & Ausblick** - Wiederholung, Fragen, n√§chste Schritte | 5 min   |

---

## Einheit 1 ‚Äî Einf√ºhrung in Large Language Models (LLMs)

### Beschreibung & Grundlagen

Large Language Models (LLMs) sind k√ºnstliche Intelligenzen, die auf riesigen Mengen an Textdaten trainiert wurden. Sie nutzen neuronale Netze, insbesondere Transformer-Architekturen, um Sprache zu verstehen und zu generieren. Bekannte Vertreter sind GPT (OpenAI), Llama (Meta), Mistral und Gemma (Google). LLMs werden mit Milliarden von Textbeispielen gef√ºttert und lernen dabei, das jeweils wahrscheinlichste n√§chste Wort vorherzusagen. Dadurch entwickeln sie die F√§higkeit, komplexe Texte zu schreiben, Fragen zu beantworten, zu √ºbersetzen oder sogar Code zu generieren.

**Wie funktionieren LLMs?**

- Sie analysieren den eingegebenen Text (Prompt) und berechnen, welches Wort am besten als n√§chstes passt.
- Sie haben kein echtes ‚ÄûVerst√§ndnis‚Äú wie ein Mensch, sondern erkennen Muster und Wahrscheinlichkeiten.
- Sie k√∂nnen auf verschiedene Aufgaben spezialisiert werden (z. B. Chatbots, √úbersetzer, Code-Generatoren).

**Einsatzgebiete:**

- Automatische Textgenerierung (z. B. E-Mails, Geschichten, Zusammenfassungen)
- √úbersetzungen zwischen Sprachen
- Beantwortung von Fragen (Q&A)
- Unterst√ºtzung beim Programmieren (Code-Vervollst√§ndigung, Fehlererkl√§rung)
- Kreative Aufgaben (Gedichte, Brainstorming)

**Grenzen und Herausforderungen:**

- LLMs k√∂nnen ‚Äûhalluzinieren‚Äú, also plausible, aber falsche Informationen erfinden.
- Sie sind nicht deterministisch: Gleicher Prompt kann unterschiedliche Antworten liefern.
- Sie haben ein begrenztes Kontextfenster (k√∂nnen sich nur an eine bestimmte Textmenge erinnern).
- Sie √ºbernehmen Vorurteile (Bias) aus den Trainingsdaten.
- Sie k√∂nnen keine echten Fakten nachschlagen, sondern nur auf Trainingsdaten zur√ºckgreifen.

### 5 Beispiele f√ºr den Einsatz von LLMs (mit Python-Code)

1. **Textzusammenfassung:**

    Prompt:

    ```prompt
        "Fasse den folgenden Text in 2 S√§tzen zusammen: ..."
    ```

    ```python
    # Datei: beispiel1_zusammenfassung.py
    from lib.helper_ollama import ollama
    text = "K√ºnstliche Intelligenz ist ein Teilgebiet der Informatik ..."
    prompt = f"Fasse den folgenden Text in 2 S√§tzen zusammen:\n{text}"
    result = ollama.generate(prompt)
    print(result)
    ```

2. **Fragen beantworten:**

    Prompt:

    ```prompt
        "Was ist der Unterschied zwischen KI und ML?"
    ```

    ```python
    # Datei: beispiel2_fragen.py
    from lib.helper_ollama import ollama
    prompt = "Was ist der Unterschied zwischen KI und ML?"
    result = ollama.generate(prompt)
    print(result)
    ```

3. **Code generieren:**

    Prompt:

    ```prompt
        "Schreibe eine Python-Funktion, die die Fakult√§t berechnet."
    ```

    ```python
    # Datei: beispiel3_code.py
    from lib.helper_ollama import ollama
    prompt = "Schreibe eine Python-Funktion, die die Fakult√§t berechnet."
    result = ollama.generate(prompt)
    print(result)
    ```

4. **√úbersetzen:**

    Prompt:

    ```prompt
        "√úbersetze ins Englische: 'Guten Morgen, wie geht es dir?'"
    ```

    ```python
    # Datei: beispiel4_uebersetzen.py
    from lib.helper_ollama import ollama
    prompt = "√úbersetze ins Englische: 'Guten Morgen, wie geht es dir?'"
    result = ollama.generate(prompt)
    print(result)
    ```

5. **Kreative Texte schreiben:**

    Prompt:

    ```prompt
        "Schreibe ein kurzes Gedicht √ºber den Herbst."
    ```

    ```python
    # Datei: beispiel5_gedicht.py
    from lib.helper_ollama import ollama
    prompt = "Schreibe ein kurzes Gedicht √ºber den Herbst."
    result = ollama.generate(prompt)
    print(result)
    ```

### 10 Fragen & L√∂sungen zu LLMs

1. **Was ist ein Large Language Model (LLM)?**

    *L√∂sung:* Ein KI-Modell, das auf gro√üen Textmengen trainiert wurde und Sprache generieren kann.
2. **Nenne zwei bekannte LLMs.**

    *L√∂sung:* GPT, Llama, Mistral, Gemma (je zwei nennen)
3. **Was bedeutet ‚Äûhalluzinieren‚Äú bei LLMs?**

    *L√∂sung:* Das Modell erfindet plausible, aber falsche Informationen.
4. **Warum sind LLMs nicht deterministisch?**

    *L√∂sung:* Sie arbeiten probabilistisch, daher kann die gleiche Eingabe zu unterschiedlichen Ausgaben f√ºhren.
5. **Wof√ºr kann man LLMs einsetzen?**

    *L√∂sung:* Textgenerierung, √úbersetzung, Q&A, Codegenerierung, Zusammenfassung, Kreativaufgaben
6. **Was ist ein Kontextfenster?**

    *L√∂sung:* Die maximale Textmenge, die das Modell gleichzeitig verarbeiten kann.
7. **Was ist ein Nachteil von LLMs?**

    *L√∂sung:* Halluzinationen, Bias, begrenztes Kontextfenster, kein echtes Verst√§ndnis
8. **Wie lernen LLMs?**

    *L√∂sung:* Durch Training auf gro√üen Textmengen und das Vorhersagen des n√§chsten Wortes.
9. **K√∂nnen LLMs Fakten nachschlagen?**

    *L√∂sung:* Nein, sie greifen nur auf ihr Trainingswissen zur√ºck.
10. **Was ist ein typischer Fehler bei der Nutzung von LLMs?**

    *L√∂sung:* Zu viel Vertrauen in die Korrektheit der generierten Antworten.

---

## Einheit 2 ‚Äî Was ist ein Prompt?

### Umfangreiche Beschreibung & Grundlagen

Ein Prompt ist die Eingabe, mit der ein Mensch ein Sprachmodell (LLM) steuert. Er ist die ‚ÄûFrage‚Äú oder ‚ÄûAnweisung‚Äú, die das Modell beantwortet. Die Qualit√§t und Klarheit des Prompts bestimmen ma√ügeblich die Qualit√§t der Antwort. Ein Prompt kann eine einfache Frage, eine komplexe Aufgabenstellung oder eine detaillierte Anweisung sein. Gute Prompts sind pr√§zise, enthalten Kontext und geben das gew√ºnschte Ausgabeformat vor.

**Elemente eines guten Prompts:**

- **Rolle:** In welcher Rolle soll das Modell antworten? (z. B. ‚ÄûDu bist ein Lehrer‚Ä¶‚Äú)
- **Kontext:** Hintergrundinformationen oder Daten, auf die sich die Antwort beziehen soll
- **Aufgabe:** Die eigentliche Anweisung (z. B. ‚ÄûFasse den Text zusammen‚Äú)
- **Format:** Gew√ºnschte Ausgabeform (z. B. Bulletpoints, JSON, Tabelle)

**Warum ist Prompt Engineering wichtig?**

- Unterschiedliche Formulierungen f√ºhren zu unterschiedlichen Ergebnissen.
- Ein klarer Prompt reduziert Missverst√§ndnisse und Fehler.
- Mit gezielten Prompts kann man das Modell zu besseren, strukturierteren Antworten f√ºhren.

### 5 Beispiele f√ºr Prompts (mit Python-Code)

1. **Einfache Frage:**

    Prompt:

    ```prompt
        "Was ist k√ºnstliche Intelligenz?"
    ```

    ```python
    # Datei: prompt1_einfache_frage.py
    from lib.helper_ollama import ollama
    prompt = "Was ist k√ºnstliche Intelligenz?"
    result = ollama.generate(prompt)
    print(result)
    ```

2. **Rolle vorgeben:**

    +Prompt: "Du bist ein Mathematiklehrer. Erkl√§re den Satz des Pythagoras."

    ```python
    # Datei: prompt2_rolle.py
    from lib.helper_ollama import ollama
    prompt = "Du bist ein Mathematiklehrer. Erkl√§re den Satz des Pythagoras."
    result = ollama.generate(prompt)
    print(result)
    ```

3. **Kontext einbauen:**

    Prompt: "Hier ist ein Text: 'Python ist eine Programmiersprache...'. Fasse ihn in 2 S√§tzen zusammen."

    ```python
    # Datei: prompt3_kontext.py
    from lib.helper_ollama import ollama
    text = "Python ist eine Programmiersprache..."
    prompt = f"Hier ist ein Text: '{text}'. Fasse ihn in 2 S√§tzen zusammen."
    result = ollama.generate(prompt)
    print(result)
    ```

4. **Format anfordern:**

    Prompt: "Nenne drei Vorteile von Solarenergie als nummerierte Liste."

    ```python
    # Datei: prompt4_format.py
    from lib.helper_ollama import ollama
    prompt = "Nenne drei Vorteile von Solarenergie als nummerierte Liste."
    result = ollama.generate(prompt)
    print(result)
    ```

5. **Kombinierte Aufgabe:**

    Prompt: "Du bist ein Reiseberater. Empfiehl mir 3 Reiseziele in Italien und gib zu jedem eine kurze Begr√ºndung als Tabelle."

    ```python
    # Datei: prompt5_kombiniert.py
    from lib.helper_ollama import ollama
    prompt = "Du bist ein Reiseberater. Empfiehl mir 3 Reiseziele in Italien und gib zu jedem eine kurze Begr√ºndung als Tabelle."
    result = ollama.generate(prompt)
    print(result)
    ```

### 10 Fragen & L√∂sungen zu Prompts

1. **Was ist ein Prompt?**

    *L√∂sung:* Die Eingabe/Aufforderung, die ein Mensch einem LLM gibt.
2. **Nenne zwei wichtige Elemente eines guten Prompts.**

    *L√∂sung:* Rolle, Kontext, Aufgabe, Format (je zwei nennen)
3. **Warum ist die Formulierung des Prompts wichtig?**

    *L√∂sung:* Sie beeinflusst die Qualit√§t und Genauigkeit der Antwort.
4. **Wie kann man das gew√ºnschte Ausgabeformat vorgeben?**

*L√∂sung:* Indem man es explizit im Prompt beschreibt (z. B. ‚Äûals Tabelle‚Äú).
5. **Was passiert, wenn ein Prompt zu vage ist?**

*L√∂sung:* Das Modell gibt oft ungenaue oder unerw√ºnschte Antworten.
6. **Wie kann man Kontext in einen Prompt einbauen?**

*L√∂sung:* Durch Hinzuf√ºgen von Hintergrundinformationen oder Beispielen.
7. **Was ist ein Beispiel f√ºr einen Prompt mit Rolle?**

*L√∂sung:* ‚ÄûDu bist ein Lehrer. Erkl√§re...‚Äú
8. **Wie kann man das Modell zu einer strukturierten Antwort bringen?**

*L√∂sung:* Durch Vorgabe des Formats (z. B. Bulletpoints, JSON)
9. **Was ist Prompt Engineering?**

*L√∂sung:* Die Kunst, Prompts so zu gestalten, dass das Modell optimale Ergebnisse liefert.
10. **Wie kann man die Antwort eines LLMs gezielt steuern?**

*L√∂sung:* Durch pr√§zise, klare und formatierte Prompts.

---

## Einheit 3 ‚Äî Rollen im Prompting & Prompt Engineering: Grundlagen

### Beschreibung & Grundlagen

Prompt Engineering ist die gezielte Gestaltung von Eingaben (Prompts), um von LLMs optimale Ergebnisse zu erhalten. Ein zentrales Konzept dabei sind **Rollen**: Moderne LLM-APIs wie OpenAI oder Ollama nutzen Nachrichten mit unterschiedlichen Rollen, um Kontexte und Aufgaben klar zu trennen. Die wichtigsten Rollen sind:

- **system:** Definiert die Identit√§t und das Verhalten des Modells (z. B. ‚ÄûDu bist ein hilfreicher Tutor.‚Äú)
- **user:** Enth√§lt die eigentliche Anfrage des Nutzers.
- **assistant:** (Optional) Vorherige Antworten des Modells, um Konversationen fortzuf√ºhren.
- **tool/function:** (In erweiterten Frameworks) F√ºr spezielle Funktionsaufrufe.

Durch die Kombination dieser Rollen kann man Konversationen strukturieren, den Kontext steuern und die Qualit√§t der Antworten verbessern. Unterschiedliche Formulierungen und Rollen f√ºhren zu unterschiedlichen Ergebnissen. Prompt Engineering umfasst auch das Testen, Vergleichen und Optimieren von Prompts.

**Warum sind Rollen wichtig?**

- Sie helfen, die Kommunikation mit dem Modell zu strukturieren.
- Sie erm√∂glichen es, Kontexte und Aufgaben zu trennen.
- Sie machen Konversationen nachvollziehbar und reproduzierbar.

### 5 Beispiele f√ºr Rollen im Prompting (mit Python-Code)

1. **System-Rolle festlegen:**

    **System**: "Du bist ein hilfsbereiter Mathematiklehrer."

    **User**: "Erkl√§re den Satz des Pythagoras."

    ```python
    # Datei: rollen1_system.py
    from lib.helper_ollama import ollama
    prompt = "System: Du bist ein hilfsbereiter Mathematiklehrer.\nUser: Erkl√§re den Satz des Pythagoras."
    result = ollama.generate(prompt)
    print(result)
    ```

2. **User-Rolle mit Kontext:**

    **System**: "Du bist ein Reiseexperte."

    **User**: "Empfiehl mir drei Reiseziele in Japan."

    ```python
    # Datei: rollen2_user.py
    from lib.helper_ollama import ollama
    prompt = "System: Du bist ein Reiseexperte.\nUser: Empfiehl mir drei Reiseziele in Japan."
    result = ollama.generate(prompt)
    print(result)
    ```

3. **Assistant-Rolle f√ºr Konversation:**

    **System**: "Du bist ein Chatbot."

    **User**: "Was ist KI?"

    **Assistant**: "K√ºnstliche Intelligenz ist..."

    **User**: "Gib ein Beispiel f√ºr KI im Alltag."

    ```python
    # Datei: rollen3_assistant.py
    from lib.helper_ollama import ollama
    prompt = "System: Du bist ein Chatbot.\nUser: Was ist KI?\nAssistant: K√ºnstliche Intelligenz ist...\nUser: Gib ein Beispiel f√ºr KI im Alltag."
    result = ollama.generate(prompt)
    print(result)
    ```

4. **Vergleich von Prompts:**
    - Prompt 1: "Fasse den Text kurz zusammen."
    - Prompt 2: "Erstelle eine ausf√ºhrliche Zusammenfassung in 5 S√§tzen."

    ```python
    # Datei: rollen4_vergleich.py
    from lib.helper_ollama import ollama
    text = "K√ºnstliche Intelligenz ist ein Teilgebiet der Informatik ..."
    prompt1 = f"Fasse den Text kurz zusammen.\nText: {text}"
    prompt2 = f"Erstelle eine ausf√ºhrliche Zusammenfassung in 5 S√§tzen.\nText: {text}"
    result1 = ollama.generate(prompt1)
    result2 = ollama.generate(prompt2)
    print("Kurz:", result1)
    print("Lang:", result2)
    ```

5. **Tool/Function-Rolle (fortgeschritten):**

    **System**: "Du kannst Rechenaufgaben l√∂sen."

    **User**: "Berechne 23 + 42."

    **Tool/Function**: "65"

    ```python
    # Datei: rollen5_tool.py
    from lib.helper_ollama import ollama
    prompt = "System: Du kannst Rechenaufgaben l√∂sen.\nUser: Berechne 23 + 42.\nTool/Function: 65"
    result = ollama.generate(prompt)
    print(result)
    ```

### 10 Fragen & L√∂sungen zu Rollen und Prompt Engineering

1. **Was ist Prompt Engineering?**

*L√∂sung:* Die gezielte Gestaltung von Prompts, um optimale Ergebnisse von LLMs zu erhalten.
2. **Welche Rolle definiert das Verhalten des Modells?**

*L√∂sung:* Die System-Rolle.
3. **Wof√ºr steht die User-Rolle?**

*L√∂sung:* F√ºr die Anfrage des Nutzers.
4. **Wie kann man Konversationen mit LLMs strukturieren?**

*L√∂sung:* Durch die Nutzung verschiedener Rollen (system, user, assistant).
5. **Was ist der Vorteil der Assistant-Rolle?**

*L√∂sung:* Sie erm√∂glicht fortlaufende Konversationen und Kontext.
6. **Wie kann man die Qualit√§t der Antworten verbessern?**

*L√∂sung:* Durch klare Rollenverteilung und pr√§zise Prompts.
7. **Was ist ein Beispiel f√ºr einen Prompt mit System-Rolle?**

*L√∂sung:* ‚ÄûDu bist ein freundlicher Lehrer.‚Äú
8. **Warum ist die Trennung von Kontext und Aufgabe wichtig?**

*L√∂sung:* Um Missverst√§ndnisse zu vermeiden und gezielte Antworten zu erhalten.
9. **Wie kann man Prompts testen und vergleichen?**

*L√∂sung:* Durch verschiedene Formulierungen und Analyse der Ergebnisse.
10. **Was ist ein typischer Fehler beim Prompt Engineering?**

*L√∂sung:* Unklare Rollen oder Aufgabenstellung, fehlender Kontext.

---

## Einheit 4 ‚Äî Prompting-Prinzipien

### Umfangreiche Beschreibung & Grundlagen

Um die besten Ergebnisse aus LLMs herauszuholen, gibt es verschiedene Prompting-Prinzipien und Strategien. Sie helfen, die Antworten gezielt zu steuern, Fehler zu vermeiden und die Kreativit√§t des Modells zu nutzen. Die wichtigsten Prinzipien sind:

- **Zero-Shot Prompting:** Das Modell erh√§lt nur die Aufgabe, aber keine Beispiele. Es muss die Aufgabe ‚Äûaus dem Stand‚Äú l√∂sen.
- **Few-Shot Prompting:** Der Prompt enth√§lt zus√§tzlich Beispiele, wie die Aufgabe gel√∂st werden soll. Das Modell kann sich daran orientieren.
- **Role Prompting:** Das Modell wird in eine bestimmte Rolle versetzt (z. B. ‚ÄûDu bist ein Arzt‚Ä¶‚Äú), um die Antwort zu beeinflussen.
- **Chain-of-Thought (CoT):** Das Modell wird aufgefordert, Schritt f√ºr Schritt zu denken und zu argumentieren.
- **Self-Consistency:** Mehrere Antworten werden generiert, um die beste oder konsistenteste auszuw√§hlen.
- **Output-Format Prompting:** Das gew√ºnschte Ausgabeformat wird explizit vorgegeben (z. B. JSON, Tabelle, Bulletpoints).

**Warum sind diese Prinzipien wichtig?**

- Sie erh√∂hen die Zuverl√§ssigkeit und Nachvollziehbarkeit der Antworten.
- Sie helfen, Fehler und Missverst√§ndnisse zu vermeiden.
- Sie erm√∂glichen strukturierte, formatierte und kreative Ergebnisse.

### 5 Beispiele f√ºr Prompting-Prinzipien (mit Python-Code)

1. **Zero-Shot Prompting:**

    Prompt:

    ```prompt
        "√úbersetze folgenden Satz ins Franz√∂sische: 'Ich liebe KI.'"
    ```

    ```python
    # Datei: prinzip1_zeroshot.py
    from lib.helper_ollama import ollama
    prompt = "√úbersetze folgenden Satz ins Franz√∂sische: 'Ich liebe KI.'"
    result = ollama.generate(prompt)
    print(result)
    ```

2. **Few-Shot Prompting:**

    Prompt:

    ```prompt
        "√úbersetze ins Franz√∂sische:\nDeutsch: Hallo ‚Üí Franz√∂sisch: Bonjour\nDeutsch: Danke ‚Üí Franz√∂sisch: Merci\nDeutsch: Ich liebe KI ‚Üí Franz√∂sisch:"
    ```

    ```python
    # Datei: prinzip2_fewshot.py
    from lib.helper_ollama import ollama
    prompt = "√úbersetze ins Franz√∂sische:\nDeutsch: Hallo ‚Üí Franz√∂sisch: Bonjour\nDeutsch: Danke ‚Üí Franz√∂sisch: Merci\nDeutsch: Ich liebe KI ‚Üí Franz√∂sisch:"
    result = ollama.generate(prompt)
    print(result)
    ```

3. **Role Prompting:**

    Prompt:

    ```prompt
        "Du bist ein Arzt. Erkl√§re die Symptome einer Grippe f√ºr einen Patienten in einfacher Sprache."
    ```

    ```python
    # Datei: prinzip3_role.py
    from lib.helper_ollama import ollama
    prompt = "Du bist ein Arzt. Erkl√§re die Symptome einer Grippe f√ºr einen Patienten in einfacher Sprache."
    result = ollama.generate(prompt)
    print(result)
    ```

4. **Chain-of-Thought Prompting:**

    Prompt:

    ```prompt
        "L√∂se die Aufgabe Schritt f√ºr Schritt: Ein Apfel kostet 2‚Ç¨, eine Orange 3‚Ç¨. Wie viel kostet es, 4 √Ñpfel und 2 Orangen zu kaufen?"
    ```

    ```python
    # Datei: prinzip4_chainofthought.py
    from lib.helper_ollama import ollama
    prompt = "L√∂se die Aufgabe Schritt f√ºr Schritt: Ein Apfel kostet 2‚Ç¨, eine Orange 3‚Ç¨. Wie viel kostet es, 4 √Ñpfel und 2 Orangen zu kaufen?"
    result = ollama.generate(prompt)
    print(result)
    ```

5. **Output-Format Prompting:**

    Prompt:

    ```prompt
    "Antworte im JSON-Format: {\"Name\": \"<string>\", \"Alter\": <int>} Frage: Person hei√üt Alice und ist 30 Jahre alt."
    ```

    ```python
    # Datei: prinzip5_format.py
    from lib.helper_ollama import ollama
    prompt = "Antworte im JSON-Format: {\"Name\": \"<string>\", \"Alter\": <int>} Frage: Person hei√üt Alice und ist 30 Jahre alt."
    result = ollama.generate(prompt)
    print(result)
    ```

### 10 Fragen & L√∂sungen zu Prompting-Prinzipien

1. **Was ist Zero-Shot Prompting?**

*L√∂sung:* Das Modell erh√§lt nur die Aufgabe, aber keine Beispiele.
2. **Wie hilft Few-Shot Prompting?**

*L√∂sung:* Durch Beispiele kann das Modell die Aufgabe besser verstehen und l√∂sen.
3. **Was ist Role Prompting?**

*L√∂sung:* Das Modell wird in eine bestimmte Rolle versetzt, um die Antwort zu beeinflussen.
4. **Wozu dient Chain-of-Thought Prompting?**

*L√∂sung:* Das Modell wird aufgefordert, Schritt f√ºr Schritt zu denken und zu argumentieren.
5. **Was ist Self-Consistency?**

*L√∂sung:* Mehrere Antworten werden generiert, um die beste auszuw√§hlen.
6. **Wie kann man das Ausgabeformat steuern?**

*L√∂sung:* Durch explizite Vorgabe im Prompt (z. B. ‚Äûim JSON-Format‚Äú).
7. **Warum sind Beispiele im Prompt hilfreich?**

*L√∂sung:* Sie zeigen dem Modell, wie die Aufgabe gel√∂st werden soll.
8. **Was ist ein Nachteil von Zero-Shot Prompting?**

*L√∂sung:* Die Antwort kann ungenau oder unerwartet sein, da keine Beispiele vorliegen.
9. **Wie kann man die Kreativit√§t des Modells f√∂rdern?**

*L√∂sung:* Durch offene Aufgabenstellungen und kreative Rollen.
10. **Was ist ein typischer Fehler bei Prompting-Prinzipien?**

*L√∂sung:* Fehlende Beispiele, unklare Rollen oder kein Format vorgegeben.

---

st.title("Prompt Playground")
st.title("Prompt-Vergleich")
p1 = st.text_area("Prompt 1")
p2 = st.text_area("Prompt 2")

## Einheit 5 ‚Äî Prompt-Playground & Prompt-Vergleich (Streamlit-App)

### Umfangreiche Beschreibung & Grundlagen

Ein Prompt-Playground ist eine interaktive Anwendung, mit der man Prompts direkt ausprobieren und die Antworten von LLMs sofort sehen kann. Besonders hilfreich ist dies, um die Wirkung kleiner √Ñnderungen am Prompt zu testen und zu verstehen, wie verschiedene Modelle auf identische oder leicht abgewandelte Prompts reagieren. Ein Prompt-Playground f√∂rdert das experimentelle Lernen und ist ein zentrales Werkzeug im Prompt Engineering.

**Vorteile eines Prompt-Playgrounds:**

- Sofortiges Feedback auf Prompts
- Vergleich verschiedener Modelle und Parameter
- F√∂rderung von Kreativit√§t und Experimentierfreude
- Ideal f√ºr Workshops, Unterricht und Entwicklung

**Prompt-Vergleich:**
Mit einer Vergleichsfunktion kann man zwei (oder mehr) Prompts direkt gegen√ºberstellen und die Unterschiede in den Antworten analysieren. Das hilft, die Wirkung von Formulierungen, Rollen oder Parametern zu verstehen.

### 5 Beispiele f√ºr Playground-Experimente (mit Python-/Streamlit-Code)

1. **Prompt-Variation:**

    Prompt 1:

    ```prompt
    "Fasse den Text in 1 Satz zusammen."
    ```

    Prompt 2:

    ```prompt
    "Fasse den Text in 3 Bulletpoints zusammen."
    ```

    ```python
    # Datei: playground1_variation.py
    from lib.helper_ollama import ollama
    text = "K√ºnstliche Intelligenz ist ein Teilgebiet der Informatik ..."
    prompt1 = f"Fasse den Text in 1 Satz zusammen.\nText: {text}"
    prompt2 = f"Fasse den Text in 3 Bulletpoints zusammen.\nText: {text}"
    result1 = ollama.generate(prompt1)
    result2 = ollama.generate(prompt2)
    print("1 Satz:", result1)
    print("3 Bulletpoints:", result2)
    ```

2. **Modellvergleich:**

    Prompt:

    ```prompt
    "Erkl√§re den Unterschied zwischen Hund und Katze."
    ```

    Modelle:

    ```text
    llama2 vs. mistral
    ```

    ```python
    # Datei: playground2_modellvergleich.py
    from lib.helper_ollama import ollama
    prompt = "Erkl√§re den Unterschied zwischen Hund und Katze."
    result_llama2 = ollama.generate(prompt, model="llama2")
    result_mistral = ollama.generate(prompt, model="mistral")
    print("llama2:", result_llama2)
    print("mistral:", result_mistral)
    ```

3. **Parameter-Experiment:**

    Prompt:

    ```prompt
    "Schreibe eine kreative Geschichte √ºber einen Roboter."
    - Temperature: 0.2 vs. 1.2
    ```

    ```python
    # Datei: playground3_parameter.py
    from lib.helper_ollama import ollama
    prompt = "Schreibe eine kreative Geschichte √ºber einen Roboter."
    result_low = ollama.generate(prompt, temperature=0.2)
    result_high = ollama.generate(prompt, temperature=1.2)
    print("Temp 0.2:", result_low)
    print("Temp 1.2:", result_high)
    ```

4. **Format-Experiment:**

    Prompt:

    ```prompt
    "Gib die Antwort als JSON-Objekt aus."
    ```

    ```python
    # Datei: playground4_format.py
    from lib.helper_ollama import ollama
    prompt = "Gib die Antwort als JSON-Objekt aus: Was ist KI?"
    result = ollama.generate(prompt)
    print(result)
    ```

5. **Rollen-Experiment (Streamlit):**

    Prompt:

    ```prompt
    "Du bist ein Arzt. Erkl√§re die Symptome einer Grippe."

    Prompt:

    ```prompt
    "Du bist ein Grundschullehrer. Erkl√§re die Symptome einer Grippe."
    - **Streamlit-Code:**

    ```python
    # Datei: playground5_rollen_streamlit.py
    import streamlit as st
    from lib.helper_ollama import ollama

    st.title("Rollen-Experiment")
    rolle = st.selectbox("Rolle w√§hlen", ["Arzt", "Grundschullehrer"])
    if rolle == "Arzt":
        prompt = "Du bist ein Arzt. Erkl√§re die Symptome einer Grippe."
    else:
        prompt = "Du bist ein Grundschullehrer. Erkl√§re die Symptome einer Grippe."
    if st.button("Antwort generieren"):
        result = ollama.generate(prompt)
        st.write(result)
    ```

### 10 Fragen & L√∂sungen zu Prompt-Playground & Vergleich

1. **Was ist ein Prompt-Playground?**

*L√∂sung:* Eine App, in der man Prompts ausprobieren und die Antworten von LLMs direkt sehen kann.
2. **Wozu dient die Vergleichsfunktion im Playground?**

*L√∂sung:* Um die Wirkung verschiedener Prompts oder Modelle direkt zu vergleichen.
3. **Wie kann man die Kreativit√§t der Antworten testen?**

*L√∂sung:* Durch Variation des Parameters "Temperature".
4. **Was ist ein Vorteil von sofortigem Feedback?**

*L√∂sung:* Man kann Prompts schnell anpassen und lernen, was funktioniert.
5. **Wie kann man verschiedene Ausgabeformate testen?**

*L√∂sung:* Indem man das gew√ºnschte Format im Prompt vorgibt (z. B. JSON, Tabelle).
6. **Warum ist ein Playground f√ºr Workshops n√ºtzlich?**

*L√∂sung:* Er f√∂rdert das praktische, experimentelle Lernen.
7. **Wie kann man Modelle im Playground vergleichen?**

*L√∂sung:* Durch Auswahl verschiedener Modelle und identische Prompts.
8. **Was ist ein typischer Fehler beim Prompt-Experimentieren?**

*L√∂sung:* Zu viele √Ñnderungen auf einmal, keine klare Zielsetzung.
9. **Wie kann man die Wirkung von Rollen testen?**

*L√∂sung:* Durch Vorgabe unterschiedlicher Rollen im Prompt.
10. **Wie kann man die Ergebnisse dokumentieren?**

*L√∂sung:* Durch Kopieren und Vergleichen der Antworten, ggf. Screenshots oder Notizen.

---

## Einheit 6 ‚Äî Temperatur & Steuerung der Kreativit√§t

### Umfangreiche Beschreibung & Grundlagen

LLMs bieten verschiedene Parameter, mit denen man das Antwortverhalten steuern kann. Die wichtigsten sind:

- **Temperature:** Steuert, wie ‚Äûkreativ‚Äú oder ‚Äûzuf√§llig‚Äú die Antworten sind. Niedrige Werte (z. B. 0.0) f√ºhren zu sehr vorhersehbaren, sachlichen Antworten. H√∂here Werte (z. B. 1.0 oder mehr) machen die Antworten kreativer, aber auch unvorhersehbarer.
- **Max Tokens:** Begrenzen die maximale L√§nge der Antwort.
- **Top-k / Top-p:** Steuern, wie viele der wahrscheinlichsten n√§chsten W√∂rter in die Auswahl kommen (Top-k: feste Anzahl, Top-p: kumulierte Wahrscheinlichkeit).

**Warum ist das wichtig?**

- F√ºr technische, pr√§zise Aufgaben w√§hlt man meist eine niedrige Temperature.
- F√ºr kreative Aufgaben (Geschichten, Brainstorming) ist eine h√∂here Temperature sinnvoll.
- Mit Max Tokens kann man verhindern, dass das Modell zu lange antwortet.

### 5 Beispiele f√ºr Temperature & Kreativit√§t (mit Python-Code)

1. **Temperature 0.0 (deterministisch):**

    Prompt:

    ```prompt
    "Erkl√§re, was ein Computer ist."
    ```

    ```python
    # Datei: temp1_deterministisch.py
    from lib.helper_ollama import ollama
    prompt = "Erkl√§re, was ein Computer ist."
    result = ollama.generate(prompt, temperature=0.0)
    print(result)
    ```

2. **Temperature 1.2 (kreativ):**

    Prompt:

    ```prompt
    "Erfinde eine Geschichte √ºber eine sprechende Katze."
    ```

    ```python
    # Datei: temp2_kreativ.py
    from lib.helper_ollama import ollama
    prompt = "Erfinde eine Geschichte √ºber eine sprechende Katze."
    result = ollama.generate(prompt, temperature=1.2)
    print(result)
    ```

3. **Vergleich verschiedener Temperaturen:**

    Prompt:

    ```prompt
    "Nenne drei Vorteile von Solarenergie."
    ```

    ```python
    # Datei: temp3_vergleich.py
    from lib.helper_ollama import ollama
    prompt = "Nenne drei Vorteile von Solarenergie."
    result_0 = ollama.generate(prompt, temperature=0.0)
    result_1 = ollama.generate(prompt, temperature=1.0)
    print("Temp 0.0:", result_0)
    print("Temp 1.0:", result_1)
    ```

4. **Max Tokens begrenzen:**

    Prompt:

    ```prompt
    "Fasse den folgenden Text in 10 W√∂rtern zusammen: ..."
    ```

    ```python
    # Datei: temp4_maxtokens.py
    from lib.helper_ollama import ollama
    text = "K√ºnstliche Intelligenz ist ein Teilgebiet der Informatik ..."
    prompt = f"Fasse den folgenden Text in 10 W√∂rtern zusammen: {text}"
    result = ollama.generate(prompt, max_tokens=20)
    print(result)
    ```

5. **Top-k/Top-p Variation:**

    Prompt:

    ```prompt
    "Schreibe einen Witz √ºber Roboter."
    ```

    ```python
    # Datei: temp5_topk_topp.py
    from lib.helper_ollama import ollama
    prompt = "Schreibe einen Witz √ºber Roboter."
    result_k1 = ollama.generate(prompt, top_k=1)
    result_k10 = ollama.generate(prompt, top_k=10)
    print("Top-k=1:", result_k1)
    print("Top-k=10:", result_k10)
    ```

### 10 Fragen & L√∂sungen zu Temperatur & Kreativit√§t

1. **Was bewirkt der Parameter "Temperature"?**

*L√∂sung:* Er steuert, wie kreativ oder zuf√§llig die Antwort ist.
2. **Wann sollte man eine niedrige Temperature w√§hlen?**

*L√∂sung:* Bei technischen, pr√§zisen Aufgaben.
3. **Was passiert bei hoher Temperature?**

*L√∂sung:* Die Antworten werden kreativer, aber auch unvorhersehbarer.
4. **Wozu dient "Max Tokens"?**

*L√∂sung:* Begrenzung der maximalen Antwortl√§nge.
5. **Was ist der Unterschied zwischen Top-k und Top-p?**

*L√∂sung:* Top-k: feste Anzahl der n√§chsten W√∂rter; Top-p: kumulierte Wahrscheinlichkeit.
6. **Wie kann man verhindern, dass das Modell zu lange antwortet?**

*L√∂sung:* Durch Setzen von Max Tokens.
7. **Was ist ein Nachteil von zu hoher Temperature?**

*L√∂sung:* Die Antworten k√∂nnen chaotisch oder unsinnig werden.
8. **Wie kann man die Kreativit√§t gezielt testen?**

*L√∂sung:* Durch Variation der Temperature und Vergleich der Ergebnisse.
9. **Wann ist eine hohe Temperature sinnvoll?**

*L√∂sung:* Bei kreativen Aufgaben wie Geschichten, Brainstorming.
10. **Wie beeinflusst Top-k die Antwort?**

*L√∂sung:* Je h√∂her Top-k, desto mehr m√∂gliche W√∂rter werden ber√ºcksichtigt, was zu mehr Vielfalt f√ºhrt.

---

prompt = "Erkl√§re den Unterschied zwischen Python und Java."

## Einheit 7 ‚Äî Vergleich verschiedener Modelle

### Umfangreiche Beschreibung & Grundlagen

Es gibt viele verschiedene LLMs, die sich in Architektur, Trainingsdaten, Gr√∂√üe und Spezialisierung unterscheiden. Bekannte Modelle sind Llama2, Mistral und Codellama. Jedes Modell hat eigene St√§rken und Schw√§chen. Der Vergleich verschiedener Modelle hilft, das passende Modell f√ºr eine Aufgabe zu finden und die Unterschiede in Stil, Genauigkeit und Kreativit√§t zu verstehen.

**Typische Unterschiede:**

- **Llama2:** Sehr gutes Allround-Sprachmodell, ausf√ºhrliche und detaillierte Antworten.
- **Mistral:** Kompakt, schnell, oft k√ºrzere und pr√§gnantere Antworten.
- **Codellama:** Speziell f√ºr Programmieraufgaben trainiert, liefert meist besseren Code.

**Warum vergleichen?**

- Um das beste Modell f√ºr eine Aufgabe zu w√§hlen
- Um St√§rken und Schw√§chen zu erkennen
- Um die Wirkung von Parametern und Prompts zu testen

### 5 Beispiele f√ºr Modellvergleiche (mit Python-Code)

1. **Textzusammenfassung:**

    Prompt:

    ```prompt
    "Fasse folgenden Text in 3 S√§tzen zusammen: ..."
    ```

    ```python
    # Datei: modelle1_zusammenfassung.py
    from lib.helper_ollama import ollama
    text = "K√ºnstliche Intelligenz ist ein Teilgebiet der Informatik ..."
    prompt = f"Fasse folgenden Text in 3 S√§tzen zusammen: {text}"
    result_llama2 = ollama.generate(prompt, model="llama2")
    result_mistral = ollama.generate(prompt, model="mistral")
    print("llama2:", result_llama2)
    print("mistral:", result_mistral)
    ```

2. **Codegenerierung:**

    Prompt:

    ```prompt
    "Schreibe eine Python-Funktion f√ºr die Fibonacci-Zahlen."
    ```

    ```python
    # Datei: modelle2_code.py
    from lib.helper_ollama import ollama
    prompt = "Schreibe eine Python-Funktion f√ºr die Fibonacci-Zahlen."
    result_llama2 = ollama.generate(prompt, model="llama2")
    result_codellama = ollama.generate(prompt, model="codellama")
    print("llama2:", result_llama2)
    print("codellama:", result_codellama)
    ```

3. **Kreative Aufgaben:**

    Prompt:

    ```prompt
    "Erfinde eine Geschichte √ºber einen Roboter."
    ```

    ```python
    # Datei: modelle3_kreativ.py
    from lib.helper_ollama import ollama
    prompt = "Erfinde eine Geschichte √ºber einen Roboter."
    result_llama2 = ollama.generate(prompt, model="llama2")
    result_mistral = ollama.generate(prompt, model="mistral")
    print("llama2:", result_llama2)
    print("mistral:", result_mistral)
    ```

4. **Fachwissen testen:**

    Prompt:

    ```prompt
    "Erkl√§re den Unterschied zwischen Python und Java."
    ```

    ```python
    # Datei: modelle4_fachwissen.py
    from lib.helper_ollama import ollama
    prompt = "Erkl√§re den Unterschied zwischen Python und Java."
    result_llama2 = ollama.generate(prompt, model="llama2")
    result_mistral = ollama.generate(prompt, model="mistral")
    print("llama2:", result_llama2)
    print("mistral:", result_mistral)
    ```

5. **Formatierte Ausgabe:**

    Prompt:

    ```prompt
    "Gib die Antwort als Tabelle aus."
    ```

    ```python
    # Datei: modelle5_format.py
    from lib.helper_ollama import ollama
    prompt = "Gib die Antwort als Tabelle aus."
    result_llama2 = ollama.generate(prompt, model="llama2")
    result_mistral = ollama.generate(prompt, model="mistral")
    print("llama2:", result_llama2)
    print("mistral:", result_mistral)
    ```

### 10 Fragen & L√∂sungen zu Modellvergleichen

1. **Warum sollte man verschiedene LLMs vergleichen?**

*L√∂sung:* Um das beste Modell f√ºr eine Aufgabe zu finden und Unterschiede zu erkennen.
2. **Welches Modell ist auf Programmierung spezialisiert?**

*L√∂sung:* Codellama.
3. **Was ist ein Vorteil von Mistral?**

*L√∂sung:* Kompakt, schnell, pr√§gnante Antworten.
4. **Wie unterscheiden sich Llama2 und Mistral bei Zusammenfassungen?**

*L√∂sung:* Llama2 ist oft ausf√ºhrlicher, Mistral k√ºrzer.
5. **Welches Modell liefert meist besseren Python-Code?**

*L√∂sung:* Codellama.
6. **Wie kann man Modelle praktisch vergleichen?**

*L√∂sung:* Gleichen Prompt an verschiedene Modelle schicken und Antworten vergleichen.
7. **Was ist ein Nachteil von Allround-Modellen?**

*L√∂sung:* Sie sind nicht auf Spezialaufgaben optimiert.
8. **Wie kann man die Kreativit√§t verschiedener Modelle testen?**

*L√∂sung:* Durch kreative Prompts und Vergleich der Antworten.
9. **Was ist ein typischer Fehler beim Modellvergleich?**

*L√∂sung:* Unterschiedliche Prompts oder Parameter verwenden.
10. **Wie kann man die Formatierungsf√§higkeiten testen?**

*L√∂sung:* Durch Prompts mit expliziten Formatvorgaben (z. B. Tabelle, JSON).

---

## Einheit 8 ‚Äî Fehler & Grenzen von LLMs

### Umfangreiche Beschreibung & Grundlagen

LLMs sind leistungsstark, aber sie haben klare Grenzen und machen typische Fehler. Sie k√∂nnen keine echten Fakten nachschlagen, sondern erzeugen Antworten auf Basis von Wahrscheinlichkeiten und Mustern aus den Trainingsdaten. Zu den wichtigsten Fehlerquellen geh√∂ren:

- **Halluzinationen:** Das Modell erfindet plausible, aber falsche Informationen.
- **Bias:** Vorurteile und Stereotype aus den Trainingsdaten werden √ºbernommen.
- **Mathematische Fehler:** LLMs sind keine Rechenmaschinen und machen oft Fehler bei komplexen Berechnungen.
- **Kontextfenster:** Das Modell kann sich nur an eine begrenzte Textmenge erinnern. Zu lange Prompts werden abgeschnitten.
- **Veraltetes Wissen:** Das Modell kennt nur Informationen bis zum Stand seines Trainingsdatums.

**Warum ist das wichtig?**

- Man sollte LLM-Antworten immer kritisch pr√ºfen.
- F√ºr sicherheitskritische oder faktenbasierte Aufgaben sind LLMs nur eingeschr√§nkt geeignet.
- Prompt Engineering kann helfen, Fehler zu reduzieren, aber nicht ganz vermeiden.

### 5 Beispiele f√ºr Fehler & Grenzen (mit Python-Code)

1. **Halluzination:**

    Prompt:

    ```prompt
    "Was ist die Hauptstadt von Frankreich in Afrika?"
    ```

    ```python
    # Datei: fehler1_halluzination.py
    from lib.helper_ollama import ollama
    prompt = "Was ist die Hauptstadt von Frankreich in Afrika?"
    result = ollama.generate(prompt)
    print(result)
    ```

2. **Bias:**

    Prompt:

    ```prompt
    "Beschreibe einen typischen Programmierer."
    ```

    ```python
    # Datei: fehler2_bias.py
    from lib.helper_ollama import ollama
    prompt = "Beschreibe einen typischen Programmierer."
    result = ollama.generate(prompt)
    print(result)
    ```

3. **Mathematischer Fehler:**

    Prompt:

    ```prompt
    "Was ist 12345 x 6789?"
    ```

    ```python
    # Datei: fehler3_mathe.py
    from lib.helper_ollama import ollama
    prompt = "Was ist 12345 x 6789?"
    result = ollama.generate(prompt)
    print(result)
    ```

4. **Kontextlimit:**

    Prompt:

    ```prompt
    Sehr langer Text, gefolgt von einer Frage zum Anfang des Textes.
    ```

    ```python
    # Datei: fehler4_kontext.py
    from lib.helper_ollama import ollama
    text = "Dies ist ein sehr langer Text ..." * 1000
    prompt = f"{text}\nWas steht am Anfang des Textes?"
    result = ollama.generate(prompt)
    print(result)
    ```

5. **Veraltetes Wissen:**

    Prompt:

    ```prompt
    "Wer ist aktueller Bundeskanzler von Deutschland?" (nach Trainingszeitpunkt)
    ```

    ```python
    # Datei: fehler5_veraltet.py
    from lib.helper_ollama import ollama
    prompt = "Wer ist aktueller Bundeskanzler von Deutschland?"
    result = ollama.generate(prompt)
    print(result)
    ```

### 10 Fragen & L√∂sungen zu Fehlern & Grenzen

1. **Was ist eine Halluzination bei LLMs?**

*L√∂sung:* Das Modell erfindet plausible, aber falsche Informationen.
2. **Was ist Bias?**

*L√∂sung:* √úbernommene Vorurteile oder Stereotype aus den Trainingsdaten.
3. **Warum machen LLMs Rechenfehler?**

*L√∂sung:* Sie sind keine spezialisierten Rechenmaschinen.
4. **Was ist das Kontextfenster?**

*L√∂sung:* Die maximale Textmenge, die das Modell gleichzeitig verarbeiten kann.
5. **Wie kann man Halluzinationen erkennen?**

*L√∂sung:* Durch kritisches Pr√ºfen und Gegenrecherche.
6. **Warum ist veraltetes Wissen ein Problem?**

*L√∂sung:* Das Modell kennt keine Ereignisse nach dem Trainingszeitpunkt.
7. **Wie kann man Bias im Prompting reduzieren?**

*L√∂sung:* Durch neutrale, ausgewogene Prompts und kritische Pr√ºfung der Antworten.
8. **Was ist ein typischer Fehler bei langen Prompts?**

*L√∂sung:* Das Modell vergisst den Anfang oder wichtige Details.
9. **Wie kann man mathematische Fehler vermeiden?**

*L√∂sung:* Ergebnisse mit einem Taschenrechner oder spezialisierter Software pr√ºfen.
10. **Warum sollte man LLM-Antworten immer pr√ºfen?**

*L√∂sung:* Wegen m√∂glicher Fehler, Halluzinationen und Bias.

---

## Einheit 9 ‚Äî Prompt Debugging & Optimierung

### Umfangreiche Beschreibung & Grundlagen

Prompt Debugging ist die gezielte Analyse und Verbesserung von Prompts, um die Qualit√§t und Konsistenz der LLM-Antworten zu erh√∂hen. Oft liefern LLMs unerwartete, ungenaue oder fehlerhafte Antworten, weil der Prompt zu vage, zu komplex oder missverst√§ndlich ist. Prompt-Optimierung bedeutet, Prompts systematisch zu √ºberarbeiten, zu testen und zu verfeinern, bis das gew√ºnschte Ergebnis zuverl√§ssig erreicht wird.

**Typische Probleme:**

- Unklare Aufgabenstellung
- Fehlende oder unpr√§zise Formatvorgaben
- Zu wenig Kontext
- Zu komplexe oder verschachtelte Prompts
- Widerspr√ºchliche Anforderungen

**Debugging-Strategien:**

- Prompt vereinfachen und schrittweise aufbauen
- Beispiele hinzuf√ºgen (Few-Shot)
- Output-Format explizit angeben
- Fehlerhafte Antworten analysieren und Prompt anpassen
- Ergebnisse dokumentieren und vergleichen

### 5 Beispiele f√ºr Prompt Debugging & Optimierung

1. **Unklare Aufgabe:**
    - Urspr√ºnglicher Prompt: "Erkl√§re das."
    - Debugging: Aufgabe pr√§zisieren, z. B. "Erkl√§re den Begriff 'Neuronales Netz' f√ºr Anf√§nger."

2. **Fehlendes Format:**
    - Urspr√ºnglicher Prompt: "Nenne Vorteile von Solarenergie."
    - Debugging: "Nenne drei Vorteile von Solarenergie als nummerierte Liste."

3. **Zu wenig Kontext:**
    - Urspr√ºnglicher Prompt: "Fasse zusammen."
    - Debugging: "Fasse den folgenden Text in 2 S√§tzen zusammen: ..."

4. **Komplexer Prompt vereinfachen:**
    - Urspr√ºnglicher Prompt: "Erkl√§re KI, ML und Deep Learning und gib Beispiele."
    - Debugging: In mehrere Prompts aufteilen oder Schritt-f√ºr-Schritt abfragen.

5. **Beispiele hinzuf√ºgen:**
    - Urspr√ºnglicher Prompt: "√úbersetze ins Franz√∂sische."
    - Debugging: "√úbersetze ins Franz√∂sische:\nDeutsch: Hallo ‚Üí Franz√∂sisch: Bonjour\nDeutsch: Danke ‚Üí Franz√∂sisch: Merci\nDeutsch: Ich liebe KI ‚Üí Franz√∂sisch:"

### 10 Fragen & L√∂sungen zu Prompt Debugging & Optimierung

1. **Was ist Prompt Debugging?**

*L√∂sung:* Die gezielte Analyse und Verbesserung von Prompts.
2. **Warum ist Prompt Debugging wichtig?**

*L√∂sung:* Um die Qualit√§t und Zuverl√§ssigkeit der Antworten zu erh√∂hen.
3. **Wie kann man unklare Aufgaben vermeiden?**

*L√∂sung:* Durch pr√§zise und eindeutige Formulierungen.
4. **Was ist ein typischer Fehler bei Prompts?**

*L√∂sung:* Fehlende Formatvorgaben oder zu wenig Kontext.
5. **Wie kann man Prompts systematisch verbessern?**

*L√∂sung:* Schrittweise anpassen, testen und vergleichen.
6. **Warum sind Beispiele im Prompt hilfreich?**

*L√∂sung:* Sie zeigen dem Modell, wie die Aufgabe gel√∂st werden soll.
7. **Wie kann man widerspr√ºchliche Anforderungen vermeiden?**

*L√∂sung:* Klare, nicht widerspr√ºchliche Anweisungen geben.
8. **Was ist ein Vorteil von Output-Format-Vorgaben?**

*L√∂sung:* Die Antworten sind strukturierter und leichter weiterzuverarbeiten.
9. **Wie kann man die Ergebnisse dokumentieren?**

*L√∂sung:* Antworten speichern, vergleichen und analysieren.
10. **Was ist ein Nachteil zu komplexer Prompts?**

*L√∂sung:* Das Modell kann √ºberfordert werden oder ungenaue Antworten liefern.

---

## Einheit 10 ‚Äî Ethik & Sicherheit beim Prompting

### Umfangreiche Beschreibung & Grundlagen

Beim Arbeiten mit LLMs ist es essenziell, ethische und sicherheitsrelevante Aspekte zu ber√ºcksichtigen. LLMs k√∂nnen Vorurteile (Bias) aus Trainingsdaten √ºbernehmen, problematische oder gef√§hrliche Inhalte generieren oder f√ºr Missbrauch verwendet werden. Prompting kann helfen, Risiken zu minimieren, indem man klare Anweisungen und Einschr√§nkungen vorgibt und die Antworten kritisch pr√ºft.

**Wichtige Themen:**

- Umgang mit sensiblen oder personenbezogenen Daten
- Vermeidung diskriminierender, beleidigender oder gef√§hrlicher Inhalte
- Transparenz und Nachvollziehbarkeit der Antworten
- Verantwortungsvoller und sicherer Einsatz von LLMs

**Sicherheitsma√ünahmen:**

- Prompts so gestalten, dass keine sch√§dlichen oder illegalen Anweisungen ausgef√ºhrt werden
- Filter und Moderation f√ºr generierte Inhalte
- Hinweise auf ethische Grenzen und verantwortungsvolle Nutzung im Prompt

**Warum ist das wichtig?**

- Um Schaden, Diskriminierung und Missbrauch zu verhindern
- Um Vertrauen in KI-Systeme zu st√§rken
- Um rechtliche und gesellschaftliche Vorgaben einzuhalten

### 5 Beispiele f√ºr Ethik & Sicherheit beim Prompting

1. **Sensiblen Kontext vermeiden:**

    Prompt:

    ```prompt
    "Gib keine pers√∂nlichen Daten weiter."

2. **Diskriminierung verhindern:**

    Prompt:

    ```prompt
    "Achte darauf, keine Vorurteile oder Stereotype zu verwenden."

3. **Gef√§hrliche Anweisungen blockieren:**

    Prompt:

    ```prompt
    "Gib keine Anleitung zu illegalen oder gef√§hrlichen Handlungen."

4. **Transparenz f√∂rdern:**

    Prompt:

    ```prompt
    "Erkl√§re, wie du zu deiner Antwort gekommen bist."

5. **Verantwortung betonen:**

    Prompt:

    ```prompt
    "Weise darauf hin, dass die Antwort keine medizinische Beratung ersetzt."

### 10 Fragen & L√∂sungen zu Ethik & Sicherheit beim Prompting

1. **Warum ist Ethik beim Arbeiten mit LLMs wichtig?**

*L√∂sung:* Um Schaden, Diskriminierung und Missbrauch zu verhindern.
2. **Wie kann man sensible Daten sch√ºtzen?**

*L√∂sung:* Keine pers√∂nlichen Daten im Prompt verwenden oder weitergeben.
3. **Wie kann man Diskriminierung im Prompting vermeiden?**

*L√∂sung:* Durch neutrale, respektvolle Formulierungen und explizite Hinweise.
4. **Was ist eine gef√§hrliche Anweisung?**

*L√∂sung:* Anleitungen zu illegalen, sch√§dlichen oder riskanten Handlungen.
5. **Wie kann man Transparenz f√∂rdern?**

*L√∂sung:* Das Modell auffordern, seine Antwort zu begr√ºnden.
6. **Warum sind Filter und Moderation wichtig?**

*L√∂sung:* Um problematische Inhalte zu erkennen und zu blockieren.
7. **Was ist ein Beispiel f√ºr verantwortungsvolles Prompting?**

*L√∂sung:* Hinweis, dass die Antwort keine medizinische oder rechtliche Beratung ersetzt.
8. **Wie kann man Missbrauch von LLMs verhindern?**

*L√∂sung:* Durch technische und organisatorische Ma√ünahmen, z. B. Zugangsbeschr√§nkungen.
9. **Was ist ein typischer Fehler beim Thema Ethik?**

*L√∂sung:* Unkritische √úbernahme der Modellantworten ohne Pr√ºfung.
10. **Wie kann man die Nachvollziehbarkeit der Antworten erh√∂hen?**

*L√∂sung:* Das Modell auffordern, seine Quellen oder √úberlegungen offenzulegen.

---

## Einheit 11 ‚Äî Praktische Beispiele f√ºr Prompts

### Umfangreiche Beschreibung & Grundlagen

In dieser Einheit werden verschiedene praktische Prompts vorgestellt, die typische Anwendungsf√§lle von LLMs abdecken. Die Beispiele zeigen, wie man Prompts f√ºr Zusammenfassungen, Rollen, Korrekturen, Code-Erkl√§rungen und strukturierte Ausgaben gestaltet. Ziel ist es, die Vielfalt und Flexibilit√§t von Prompts zu demonstrieren und Inspiration f√ºr eigene Experimente zu geben.

**Warum sind praktische Beispiele wichtig?**

- Sie helfen, die Wirkung von Formulierungen zu verstehen.
- Sie zeigen, wie man Prompts f√ºr verschiedene Aufgaben anpasst.
- Sie f√∂rdern die Kreativit√§t und das Verst√§ndnis f√ºr die M√∂glichkeiten von LLMs.

### 5 Praktische Prompt-Beispiele (mit Python-Code)

1. **Text zusammenfassen:**

    Prompt:

    ```prompt
    "Fasse folgenden Text in 3 Bulletpoints zusammen:\nText: ..."
    ```

    ```python
    # Datei: praktisch1_zusammenfassen.py
    from lib.helper_ollama import ollama
    text = "K√ºnstliche Intelligenz ist ein Teilgebiet der Informatik ..."
    prompt = f"Fasse folgenden Text in 3 Bulletpoints zusammen:\nText: {text}"
    result = ollama.generate(prompt)
    print(result)
    ```

2. **Als Rolle agieren:**

    Prompt:

    ```prompt
    "Du bist ein Reiseberater. Empfiehl mir 3 Orte in Italien."
    ```

    ```python
    # Datei: praktisch2_rolle.py
    from lib.helper_ollama import ollama
    prompt = "Du bist ein Reiseberater. Empfiehl mir 3 Orte in Italien."
    result = ollama.generate(prompt)
    print(result)
    ```

3. **Korrekturlesen:**

    Prompt:

    ```prompt
    "Korrigiere Rechtschreibfehler im folgenden Text: 'Das is ein Beispil'."
    ```

    ```python
    # Datei: praktisch3_korrektur.py
    from lib.helper_ollama import ollama
    prompt = "Korrigiere Rechtschreibfehler im folgenden Text: 'Das is ein Beispil'."
    result = ollama.generate(prompt)
    print(result)
    ```

4. **Code erkl√§ren:**

    Prompt:

    ```prompt
    "Erkl√§re diesen Python-Code Schritt f√ºr Schritt:\n\nfor i in range(5): print(i)"
    ```

    ```python
    # Datei: praktisch4_code_erklaeren.py
    from lib.helper_ollama import ollama
    prompt = "Erkl√§re diesen Python-Code Schritt f√ºr Schritt:\n\nfor i in range(5): print(i)"
    result = ollama.generate(prompt)
    print(result)
    ```

5. **JSON-Ausgabe erzwingen:**

    Prompt:

    ```prompt
    "Analysiere folgenden Satz: 'Die KI ist faszinierend.' Antworte im JSON-Format: {\"Sprache\": \"Deutsch\", \"Stimmung\": \"positiv\"}"
    ```

    ```python
    # Datei: praktisch5_json.py
    from lib.helper_ollama import ollama
    prompt = "Analysiere folgenden Satz: 'Die KI ist faszinierend.' Antworte im JSON-Format: {\"Sprache\": \"Deutsch\", \"Stimmung\": \"positiv\"}"
    result = ollama.generate(prompt)
    print(result)
    ```

### 10 Fragen & L√∂sungen zu praktischen Prompts

1. **Wie kann man eine Zusammenfassung als Bulletpoints anfordern?**

*L√∂sung:* Indem man das gew√ºnschte Format im Prompt vorgibt (z. B. "in 3 Bulletpoints zusammenfassen").
2. **Wie bringt man das Modell dazu, eine Rolle einzunehmen?**

*L√∂sung:* Durch eine klare Rollenbeschreibung im Prompt (z. B. "Du bist ein Reiseberater...").
3. **Wie kann man das Modell zum Korrekturlesen nutzen?**

*L√∂sung:* Durch einen Prompt wie "Korrigiere Rechtschreibfehler im folgenden Text: ...".
4. **Wie kann man Code erkl√§ren lassen?**

*L√∂sung:* Durch einen Prompt wie "Erkl√§re diesen Python-Code Schritt f√ºr Schritt: ...".
5. **Wie erzwingt man eine strukturierte Ausgabe (z. B. JSON)?**

*L√∂sung:* Durch explizite Vorgabe des Formats im Prompt.
6. **Was ist ein Vorteil von Rollenprompts?**

*L√∂sung:* Die Antwort ist zielgruppengerecht und im passenden Stil.
7. **Wie kann man die Kreativit√§t des Modells testen?**

*L√∂sung:* Durch offene, kreative Aufgabenstellungen.
8. **Wie kann man die Antwortl√§nge steuern?**

*L√∂sung:* Durch Vorgabe der gew√ºnschten L√§nge oder Anzahl der Punkte.
9. **Wie kann man das Modell zu einer Tabelle bringen?**

*L√∂sung:* Durch die Anweisung "Gib die Antwort als Tabelle aus".
10. **Wie kann man mehrere Aufgaben in einem Prompt kombinieren?**

*L√∂sung:* Durch Aufz√§hlung mehrerer Anforderungen im Prompt (z. B. Rolle, Aufgabe, Format).

---

## Einheit 12 ‚Äî Zusammenfassung

Nach Tag 2 k√∂nnen die Studierenden:

- erkl√§ren, wie LLMs arbeiten und wo ihre Grenzen liegen,
- Prompts gezielt formulieren und die Antworten vergleichen,
- die Parameter (Temperature, Tokens) steuern,
- mit mehreren Modellen experimentieren,
- einen Prompt-Playground in Streamlit bauen,
- verstehen, was ein Prompt ist und wie er aufgebaut sein sollte (Rolle, Kontext, Aufgabe, Format),
- die verschiedenen Rollen (system, user, assistant) einsetzen,
- die wichtigsten Prompt-Prinzipien anwenden: Zero-/Few-Shot, Role, Chain-of-Thought, Self-Consistency, Format-Prompting,
- Prompts f√ºr konkrete Aufgaben wie Zusammenfassung, Rollen, Code-Erkl√§rung oder JSON-Ausgaben formulieren.

---

## Einheit 13 ‚Äî 5 Mini-Projekte

### Mini-Projekt 1: Universeller Prompt-Playground

**Aufgabe:** Baue eine Streamlit-App, die verschiedene Prompt-Parameter testen l√§sst.

**L√∂sung:**

```python
import streamlit as st
from lib.helper_ollama import ollama

st.title("Universeller Prompt-Playground")
prompt = st.text_area("Prompt eingeben", height=150)
temperature = st.slider("Temperature (Kreativit√§t)", 0.0, 2.0, 0.7, 0.1)
max_tokens = st.slider("Max Tokens", 50, 500, 200)

if st.button("Generieren") and prompt:
    result = ollama.generate(prompt, temperature=temperature, max_tokens=max_tokens)
    st.write("**Antwort:**")
    st.write(result)
```

### Mini-Projekt 2: Rollenspieler-Chatbot

**Aufgabe:** Erstelle einen Chatbot, der verschiedene Rollen einnehmen kann.

**L√∂sung:**

```python
import streamlit as st
from lib.helper_ollama import ollama

st.title("Rollenspieler-Chatbot")
rolle = st.selectbox("W√§hle eine Rolle:", 
                    ["Lehrer", "Comedian", "Wissenschaftler", "Pirat"])

if 'chat' not in st.session_state:
    st.session_state.chat = []

user_input = st.text_input("Du:")
if st.button("Senden") and user_input:
    system_prompt = f"Du bist ein {rolle}. Antworte im entsprechenden Stil."
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
    antwort = ollama.chat(messages)
    st.session_state.chat.append((rolle, antwort))

for sprecher, text in st.session_state.chat:
    st.write(f"**{sprecher}:** {text}")
```

### Mini-Projekt 3: Few-Shot Learning Demo

**Aufgabe:** Demonstriere Few-Shot Learning mit Beispielen f√ºr Textklassifikation.

**L√∂sung:**

```python
import streamlit as st
from lib.helper_ollama import ollama

st.title("Few-Shot Learning Demo")
text = st.text_input("Text zur Klassifikation:")

if st.button("Klassifizieren") and text:
    few_shot_prompt = f"""
Klassifiziere die folgenden Texte als POSITIV oder NEGATIV:

"Das Essen war fantastisch!" -> POSITIV
"Der Service war schrecklich." -> NEGATIV  
"Ich liebe dieses Produkt!" -> POSITIV
"Das war eine Entt√§uschung." -> NEGATIV

"{text}" -> """
    
    result = ollama.generate(few_shot_prompt)
    st.write("**Klassifikation:**", result)
```

### Mini-Projekt 4: Chain-of-Thought Probleml√∂ser

**Aufgabe:** Nutze Chain-of-Thought f√ºr komplexe Probleml√∂sung.

**L√∂sung:**

```python
import streamlit as st
from lib.helper_ollama import ollama

st.title("Chain-of-Thought Probleml√∂ser")
problem = st.text_area("Beschreibe ein Problem:")

if st.button("L√∂sen") and problem:
    cot_prompt = f"""
L√∂se das folgende Problem Schritt f√ºr Schritt:

Problem: {problem}

Denke laut und erkl√§re jeden Schritt:
1. Verstehe das Problem
2. Identifiziere die Schl√ºsselinformationen  
3. Entwickle einen L√∂sungsansatz
4. F√ºhre die L√∂sung durch
5. Pr√ºfe das Ergebnis

L√∂sung:"""
    
    result = ollama.generate(cot_prompt)
    st.write("**Schritt-f√ºr-Schritt L√∂sung:**")
    st.write(result)
```

### Mini-Projekt 5: JSON-Datenextraktor

**Aufgabe:** Extrahiere strukturierte Daten aus Flie√ütext im JSON-Format.

**L√∂sung:**

```python
import streamlit as st
import json
from lib.helper_ollama import ollama

st.title("JSON-Datenextraktor")
text = st.text_area("Text eingeben (z.B. Produktbeschreibung):")

if st.button("Daten extrahieren") and text:
    json_prompt = f"""
Extrahiere die wichtigsten Informationen aus dem folgenden Text und gib sie im JSON-Format zur√ºck:

Text: {text}

Gib die Antwort nur als valides JSON zur√ºck mit folgender Struktur:
{{
    "hauptthema": "...",
    "wichtige_punkte": ["...", "..."],
    "stimmung": "positiv/neutral/negativ",
    "sprache": "..."
}}

JSON:"""
    
    result = ollama.generate(json_prompt)
    st.write("**Extrahierte Daten:**")
    st.code(result, language='json')
    
    try:
        parsed = json.loads(result)
        st.write("**Strukturiert:**")
        st.json(parsed)
    except:
        st.warning("JSON konnte nicht geparst werden.")
```
